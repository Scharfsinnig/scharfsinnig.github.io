<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>RAG系统性调研：范式、方法与工程化路径 | SCHARFSINNIGの博客</title><meta name="keywords" content="RAG,检索增强生成,LLM,信息检索"><meta name="author" content="Scharfsinnig"><meta name="copyright" content="Scharfsinnig"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="系统梳理 RAG 的研究脉络、三大范式（简单&#x2F;高级&#x2F;模块化）与工程化实践，覆盖检索、生成、编排与评估要点，并附代表性方法与参考资料。">
<meta property="og:type" content="article">
<meta property="og:title" content="RAG系统性调研：范式、方法与工程化路径">
<meta property="og:url" content="https://blog.whispergenie.com/post/64926.html">
<meta property="og:site_name" content="SCHARFSINNIGの博客">
<meta property="og:description" content="系统梳理 RAG 的研究脉络、三大范式（简单&#x2F;高级&#x2F;模块化）与工程化实践，覆盖检索、生成、编排与评估要点，并附代表性方法与参考资料。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.whispergenie.com/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-01.png">
<meta property="article:published_time" content="2025-11-10T09:03:56.000Z">
<meta property="article:modified_time" content="2025-11-12T03:08:24.903Z">
<meta property="article:author" content="Scharfsinnig">
<meta property="article:tag" content="RAG">
<meta property="article:tag" content="检索增强生成">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="信息检索">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.whispergenie.com/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-01.png"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Scharfsinnig/scharfsinnig.github.io@main/images/icons/favicon.png"><link rel="canonical" href="https://blog.whispergenie.com/post/64926"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//www.clarity.ms"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0.27/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d31df79654e408080cd6cc34a11919ee";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script><script>(function(c,l,a,r,i,t,y){
    c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
    t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
    y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
})(window, document, "clarity", "script", "tp16hgqun6");
</script><script defer="defer" src="https://cloud.umami.is/script.js" data-website-id="1b2c08f8-fc2f-47ca-9d18-77a59492990e"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":100,"languages":{"author":"作者: Scharfsinnig","link":"链接: ","source":"来源: SCHARFSINNIGの博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2.1.2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'RAG系统性调研：范式、方法与工程化路径',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-12 11:08:24'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#1a1a2e')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><script>var meting_api="https://api.injahow.cn/meting/?server=:server&type=:type&id=:id&r=:r";</script><script defer data-domain="blog.whispergenie.com" src="https://plausible.io/js/script.js"></script><link rel="stylesheet" href="/css/mermaid-modal.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatars/avator.png" onerror="this.onerror=null;this.src='/img/index/avator.png'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 充电站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://ocw.mit.edu/"><i class="fa-fw fa fa-university"></i><span> MIT 公开课</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.youtube.com/"><i class="fa-fw fab fa-youtube"></i><span> YouTube</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://see.stanford.edu/"><i class="fa-fw fa fa-university"></i><span> 斯坦福公开课</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://huggingface.co/"><i class="fa-fw fas fa-robot"></i><span> Hugging Face</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://modelscope.cn/"><i class="fa-fw fas fa-cubes"></i><span> ModelScope</span></a></li><li><a class="site-page child" href="/bloggers/"><i class="fa-fw fa fa-rss"></i><span> 优秀博主博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://twitter.com/"><i class="fa-fw fab fa-twitter"></i><span> X/Twitter</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言信箱</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于博主</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-tools"></i><span> 工具</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-01.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">SCHARFSINNIGの博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 充电站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://ocw.mit.edu/"><i class="fa-fw fa fa-university"></i><span> MIT 公开课</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.youtube.com/"><i class="fa-fw fab fa-youtube"></i><span> YouTube</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://see.stanford.edu/"><i class="fa-fw fa fa-university"></i><span> 斯坦福公开课</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://huggingface.co/"><i class="fa-fw fas fa-robot"></i><span> Hugging Face</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://modelscope.cn/"><i class="fa-fw fas fa-cubes"></i><span> ModelScope</span></a></li><li><a class="site-page child" href="/bloggers/"><i class="fa-fw fa fa-rss"></i><span> 优秀博主博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://twitter.com/"><i class="fa-fw fab fa-twitter"></i><span> X/Twitter</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言信箱</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于博主</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-tools"></i><span> 工具</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">RAG系统性调研：范式、方法与工程化路径</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-10T09:03:56.000Z" title="发表于 2025-11-10 17:03:56">2025-11-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-12T03:08:24.903Z" title="更新于 2025-11-12 11:08:24">2025-11-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/">技术洞察</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>28分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="检索增强生成"><a href="#检索增强生成" class="headerlink" title="检索增强生成"></a>检索增强生成</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>结合信息检索和生成式模型的自然语言处理技术。通过引入外部知识库，提升生成结果的准确性。</p>
<ol>
<li>因为是系统性总结，那么有两种方式，一种是自上而下，一种是自下而上。</li>
<li>自上而下的方式就是通过这个领域的研究脉络，分析针对这个问题有怎么一套范式去解决里面的问题，那么对于范式中每一个细分的操作又有哪些优化的手段、研究的成果，以及如何评价这个成果的有效性，最后在工业界落地又是怎么样的。这种方式可以根据要点推进，以论文作为时间线比较明晰。</li>
<li>自下而上的方式，通过各个阶段出圈的代表成果（开源框架+产品），从不同的功能应用中总结出不同的应用处理范式，评估方法。这种比较散，需要对各个框架有全面的了解，也需要有多面的任务处理经验才能得到结论。</li>
</ol>
<h2 id="学术研究脉络"><a href="#学术研究脉络" class="headerlink" title="学术研究脉络"></a>学术研究脉络</h2><h3 id="开山论文"><a href="#开山论文" class="headerlink" title="开山论文"></a>开山论文</h3><h4 id="Retrieval-augmented-generation-for-knowledge-intensive-NLP-tasks"><a href="#Retrieval-augmented-generation-for-knowledge-intensive-NLP-tasks" class="headerlink" title="Retrieval-augmented generation for knowledge-intensive NLP tasks"></a>Retrieval-augmented generation for knowledge-intensive NLP tasks</h4><blockquote>
<p>2020年5月22日 提交至Arxiv 后被NIPS接受 Facebook AI Research&amp;伦敦大学&amp;纽约大学</p>
</blockquote>
<h5 id="问题的背景"><a href="#问题的背景" class="headerlink" title="问题的背景"></a>问题的背景</h5><p>通用语料基础上预训练的大语言模型在知识密集型任务（比如开放域问答）的表现不如领域模型，并且缺乏有效更新知识做出决策的手段。</p>
<h5 id="提出的方法"><a href="#提出的方法" class="headerlink" title="提出的方法"></a>提出的方法</h5><ol>
<li>检索器+生成器结构。先构建向量数据库，将知识切分放入向量库中，使用编码向量作为索引，请求来了，将请求向量化匹配索引，根据相似度选择知识块，将query拼接知识块作为输入，让大模型产生合理的回答。</li>
<li>检索器使用双编码器结构，两个不同的bert分别得到问题和文档的向量，向量检索的过程就是对问题和文档的向量求取内积作为相关度量，返回和问题向量内积最大、和问题最相关的前K个文档。<ul>
<li>文档的定义：互不重叠的包含100个单词的块</li>
<li>使用FAISS构建向量库，存放21m数量的维基百科文档</li>
</ul>
</li>
<li>生成器使用了Meta发布的大语言模型BART-Large，直接将问题和检索器返回的相关文档拼接在一起作为生成器的输入。<ul>
<li>5、10个文档实验无明显差异</li>
</ul>
</li>
<li>训练方式：训练数据是问题、回答对集合，对检索器和生成器进行端到端的联合训练，但是冻结了文档编码器的参数，目的是避免修改索引。训练数据规模未提，开源项目已关闭。</li>
</ol>
<h5 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h5><p>开放域问答做到了sota，其他任务比训练前基座模型有明显的提升。</p>
<h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><h4 id="Retrieval-Augmented-Generation-for-Large-Language-Models-A-Survey"><a href="#Retrieval-Augmented-Generation-for-Large-Language-Models-A-Survey" class="headerlink" title="Retrieval-Augmented Generation for Large Language Models: A Survey"></a>Retrieval-Augmented Generation for Large Language Models: A Survey</h4><blockquote>
<p>2023.12.18 同济&amp;复旦 主要总结讨论了RAG三个范式，以及在各个阶段优化RAG效果的方法，同时包含少部分部分评估、多模态RAG的内容。</p>
</blockquote>
<h4 id="Graph-Retrieval-Augmented-Generation-A-Survey"><a href="#Graph-Retrieval-Augmented-Generation-A-Survey" class="headerlink" title="Graph Retrieval-Augmented Generation: A Survey"></a>Graph Retrieval-Augmented Generation: A Survey</h4><blockquote>
<p>2024 年 8 月 15 日 提交至Arxiv 北大&amp;浙大&amp;蚂蚁 主要讨论在索引、检索、生成三个阶段如何应用图神经网络、知识图谱，包含部分工业界GraphRAG系统，应用领域，评估标准。 key words：GraphRAG</p>
</blockquote>
<h4 id="A-Comprehensive-Survey-of-Retrieval-Augmented-Generation-RAG-Evolution-Current-Landscape-and-Future-Directions"><a href="#A-Comprehensive-Survey-of-Retrieval-Augmented-Generation-RAG-Evolution-Current-Landscape-and-Future-Directions" class="headerlink" title="A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions"></a>A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions</h4><blockquote>
<p>2024 年 10 月 03 日 提交至 Arxiv 卡内基梅隆大学。主要介绍了多模态&#x2F;跨模态 RAG 方法，以及一些科研性质的 RAG 框架。</p>
</blockquote>
<h4 id="Agentic-RAG"><a href="#Agentic-RAG" class="headerlink" title="Agentic RAG"></a>Agentic RAG</h4><p>将 RAG 作为 Agent 的可插拔“检索-推理”工具链，按查询意图动态组合不同索引&#x2F;检索&#x2F;后处理策略（如：向量检索回答事实、摘要索引回答综述、图查询回答关系、多跳推理配合工具使用）。<br>参考与延伸：</p>
<ul>
<li>Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG（arXiv 2025）：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.09136">https://arxiv.org/abs/2501.09136</a></li>
<li>实践要点：路由（任务&#x2F;域&#x2F;模态）、多索引选择、带反馈的循环（反思-重检索-重生成）、工具选择器与安全约束。</li>
</ul>
<h2 id="RAG范式（流程-amp-整体结构）"><a href="#RAG范式（流程-amp-整体结构）" class="headerlink" title="RAG范式（流程&amp;整体结构）"></a>RAG范式（流程&amp;整体结构）</h2><blockquote>
<ol>
<li>最终的目标是增强LLM生成的结果准确性</li>
<li>研究过程是<strong>从文本RAG到多模RAG，从分散的多文档到结构化多跳理解</strong>（23-24年rag综述）</li>
<li>三个范式之间是继承与发展的关系，高级RAG是模块化RAG的一种特例，而朴素RAG则是高级RAG的一种特例。</li>
</ol>
</blockquote>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-01.png"></p>
<h3 id="简单RAG（Naive-RAG）"><a href="#简单RAG（Naive-RAG）" class="headerlink" title="简单RAG（Naive RAG）"></a>简单RAG（Naive RAG）</h3><h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><ol>
<li>索引（Indexing）<ul>
<li>*<strong>数据清理和提取</strong>：从不同格式（PDF、HTML、Word、Markdown）中提取原始数据，并将其转换为统一的纯文本格式。</li>
<li><strong>文本分块</strong>：将文本分割成更小的、可处理的块（chunk），以适应语言模型的上下文限制，降低噪声影响。</li>
<li><strong>嵌入编码</strong>：使用嵌入模型将每个块转换为向量表示。</li>
<li><strong>存储</strong>：将向量存储在数据库中，以便于高效检索。</li>
</ul>
</li>
<li>检索（Retrieval）<ul>
<li><strong>查询向量化</strong>：接收到用户查询后，使用与索引阶段相同的编码模型将查询转换为向量表示。</li>
<li><strong>相似性计算</strong>：计算查询向量与索引中块向量的相似性得分。</li>
<li><strong>检索Top-K块</strong>：根据相似性得分选择与查询最相关的Top-K个块。</li>
<li><strong>扩展上下文</strong>：选定的Top-K块作为上下文添加到生成阶段的prompt中，形成一个连贯的提示。</li>
</ul>
</li>
<li>生成（Generation）<ul>
<li><strong>生成回复</strong>：大语言模型基于提示生成答复，可能结合固有的知识或限制于提供文档中的信息。</li>
<li><strong>多轮对话支持</strong>：如果有对话历史，可以将其整合到提示中，实现连续的多轮对话。</li>
</ul>
</li>
</ol>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><blockquote>
<p>主要集中在<strong>检索</strong>、<strong>生成</strong>、<strong>增强</strong> 三个方面,实际上应用过程中如何构建有效的语义索引也存在问题，但是这个不仅仅是当前范式的问题，暂按下不表。</p>
</blockquote>
<ol>
<li>检索问题：准确率与召回率的冲突<ul>
<li>在检索阶段，系统通常需要在“准确率”（precision）和“召回率”（recall）之间做权衡。高召回率意味着系统尽量找出所有可能相关的内容，但这往往会导致包含一些不相关的信息。而高准确率则倾向于检索到更精确的信息，但可能会漏掉一些有用的内容。</li>
<li>举例：假设你正在检索一个关于“人工智能在医疗中的应用”的问题。系统如果以高召回率为目标，可能会检索到大量的文献，包括一些与主题无关的内容，比如与医疗无关的人工智能技术讨论。</li>
<li>如果系统提高准确率，只检索到非常精确的关于医疗中人工智能应用的文章，可能会漏掉一些相关但没有完全匹配的文献，这就导致了召回率的下降。</li>
<li>影响：这种冲突可能导致两种问题：<ol>
<li>检索到不相关的内容，影响后续生成阶段的质量。</li>
<li>检索不到关键信息，导致生成的回答不全面。</li>
</ol>
</li>
</ul>
</li>
<li>生成困难：幻觉问题<ul>
<li>问题：在生成阶段，模型有时会产生所谓的“幻觉”。这意味着，生成的内容虽然看起来合理，但实际上并没有在检索到的上下文中出现过。这通常是因为模型会根据其训练时学到的知识和模式生成内容，而这些内容可能与检索到的实际信息不符。</li>
<li>举例：假设用户查询“2023年诺贝尔经济学奖得主是谁？”模型检索到了一些有关诺贝尔奖的文献，但它可能会生成一个虚假的答案，比如错误地指出某个历史人物是2023年的得主，而该人物并没有在检索到的文献中提到。这种幻觉问题会影响模型的可靠性，尤其是在事实性回答上。</li>
<li>影响：幻觉问题可能导致生成的答案不真实、不准确，降低模型的质量和可信度。此外，生成的内容可能会因为毒性、偏见等问题变得不适合发布。</li>
</ul>
</li>
<li>增强障碍：任务整合问题<ul>
<li>问题：在将检索到的信息与特定任务（如问答、文档摘要等）整合时，可能会出现不连贯或不一致的输出。尤其是当从多个来源检索到相似信息时，模型有时难以整合这些信息，从而导致冗余或重复的答案。</li>
<li>举例：假设用户询问“如何修复我的车？”模型可能从两篇文章中分别检索到“检查油箱”与“检查发动机”，而这些信息虽然是相关的，但如果生成的回答没有有效整合这两条信息，就可能生成一个只包含其中一部分的答案，或者冗余地重复“检查油箱”和“检查发动机”。在复杂任务中，如需要跨领域知识的场景，单一检索可能无法涵盖所有必需的信息，导致输出不完整或不连贯。</li>
<li>影响：这种整合困难可能导致最终输出缺乏连贯性，甚至引发信息重复或遗漏，降低用户体验。</li>
</ul>
</li>
<li>过度依赖：生成模型的局限性<ul>
<li>问题：生成模型可能过度依赖检索到的增强信息，仅仅将检索到的内容作为输出，而不做进一步的推理或综合。模型可能只是复述或简单地结合检索信息，缺乏真正的洞察或创造性。</li>
<li>举例：假设用户询问“如何改善团队合作？”模型可能检索到关于团队合作的一些基本建议，如“增加沟通”和“设定目标”。如果模型只简单地把这些建议拼凑在一起，而没有进一步扩展或结合上下文中的特定细节，就会得到一个很基础、缺乏深度的回答：“加强沟通和设定目标有助于团队合作。”</li>
<li>这种过度依赖检索信息的问题限制了模型的创造力和深度，使得回答显得平淡而缺乏见解。</li>
</ul>
</li>
</ol>
<h4 id="代表性工作"><a href="#代表性工作" class="headerlink" title="代表性工作"></a>代表性工作</h4><ul>
<li>RAG（Lewis et al., 2020）：经典“检索器 + 生成器”框架；开放域问答显著提升。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a></li>
<li>REALM（Guu et al., 2020）：预训练阶段即引入可检索知识；端到端优化检索器。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.08909">https://arxiv.org/abs/2002.08909</a></li>
<li>DPR（Karpukhin et al., 2020）：双塔密集检索奠基作，成为后续 RAG 标配。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.04906">https://arxiv.org/abs/2004.04906</a></li>
<li>FiD（Izacard &amp; Grave, 2020）：Fusion-in-Decoder，把多段文档在解码端融合，强生成器范式。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.12166">https://arxiv.org/abs/2007.12166</a></li>
<li>RETRO（DeepMind, 2021&#x2F;22）：训练时检索（retrieval-augmented pretraining），参数更小也能获益。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.04426">https://arxiv.org/abs/2112.04426</a></li>
<li>ATLAS（Izacard et al., 2022）：few-shot + 检索，低监督场景表现强。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.03299">https://arxiv.org/abs/2208.03299</a></li>
<li>HyDE（Gao et al., 2022）：假设文档增强检索，零样本场景常用。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.10496">https://arxiv.org/abs/2212.10496</a></li>
<li>RAG-Fusion：多查询 + 互反排名融合（RRF），稳健提高召回与覆盖。RRF 介绍：<a target="_blank" rel="noopener" href="https://plg.uwaterloo.ca/~gvcormac/rrf.pdf">https://plg.uwaterloo.ca/~gvcormac/rrf.pdf</a></li>
<li>Self-RAG（Asai et al., 2023）：生成-检索-自我反思一体化流程。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.11511">https://arxiv.org/abs/2310.11511</a></li>
<li>C-RAG &#x2F; RAPTOR &#x2F; MemoRAG（2024）：面向“纠错&#x2F;层次检索树&#x2F;记忆”的工程化增强，详见文末“新型 RAG 范式”。</li>
</ul>
<h3 id="高级RAG（Advanced-RAG）"><a href="#高级RAG（Advanced-RAG）" class="headerlink" title="高级RAG（Advanced RAG）"></a>高级RAG（Advanced RAG）</h3><blockquote>
<p>在 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation for Large Language Models: A Survey</a>这篇论文中，相比于简单RAG，添加了两个模块，预检索过程（Pre-Retrieval Process） 和 后检索过程（Post-Retrieval Process），对整体进行了优化，主要目的是提高被索引内容的质量。 实际应用过程中也会根据需要，判断是否用思维链的手段优化生成。</p>
</blockquote>
<h4 id="预检索过程（Pre-Retrieval-Process）"><a href="#预检索过程（Pre-Retrieval-Process）" class="headerlink" title="预检索过程（Pre-Retrieval Process）"></a>预检索过程（Pre-Retrieval Process）</h4><h5 id="建立索引"><a href="#建立索引" class="headerlink" title="建立索引"></a>建立索引</h5><blockquote>
<p>实际上要关注非结构化、半结构化、结构化数据处理手段。</p>
</blockquote>
<ol>
<li>索引降噪：去除无效成分，突出核心知识，减少噪音干扰。目的是使用关键内容。比如知识库存放qa对，索引不用保留所有q的内容，保留q的关键词。</li>
<li>知识切分：通过语义理解小模型将长文本按语义切分成小块，避免核心知识丧失或语义截断。规则类切分（比如标题）+NLP语义切分（比如预测句子是否是分段的边界）。</li>
<li>增加元数据：增加如内容摘要、时间戳、用户可能问题等附加信息，增强知识库的丰富性，不需要向量化，使用机器学习手段进行稀疏检索。</li>
</ol>
<h5 id="处理查询"><a href="#处理查询" class="headerlink" title="处理查询"></a>处理查询</h5><ol>
<li>Query改写：将用户的原始问题转化为适合知识库检索的问题。<ul>
<li>Query分解：将问题拆分成多个子问题。</li>
<li>Query扩展：将问题转化为多种问法。</li>
</ul>
</li>
</ol>
<h4 id="后检索过程（Post-Retrieval-Process）"><a href="#后检索过程（Post-Retrieval-Process）" class="headerlink" title="后检索过程（Post-Retrieval Process）"></a>后检索过程（Post-Retrieval Process）</h4><h5 id="提示压缩"><a href="#提示压缩" class="headerlink" title="提示压缩"></a>提示压缩</h5><p>在输入大模型前，删除无关内容并突出重要上下文，减少冗余信息对大模型的干扰。（摘要）</p>
<h5 id="重排序"><a href="#重排序" class="headerlink" title="重排序"></a>重排序</h5><blockquote>
<p>从 RAG 阶段看，前面的检索过程可能是关键词匹配，可能是嵌入向量匹配（双编码器）；重排序阶段可能面临的是多轮对话的多个检索结果，是一个策略阶段。从技术实现看，检索阶段常用双编码器+相似度计算，重排序常用交叉编码器直接输出相似度。双编码器不够细致（未与生成器联合训练），较难感知文档细节。</p>
</blockquote>
<p>使用排序模型重新计算检索内容的相关性得分，即精排，考虑查询意图、用户历史行为、上下文信息等因素。</p>
<h3 id="模块化RAG（Modular-RAG）"><a href="#模块化RAG（Modular-RAG）" class="headerlink" title="模块化RAG（Modular RAG）"></a>模块化RAG（Modular RAG）</h3><blockquote>
<p>模块化RAG是高级RAG的特例，主要是优化了RAG框架的管理，拥有更清晰的模块划分和更直观的系统管理。 模块化RAG包括六个环节：索引、检索前、检索中、检索后、生成和编排。每个环节的功能被进一步模块化。 模块化的好处是系统能在关键节点进行动态决策，依据先前结果选择后续步骤，即根据具体的查询选择合适的知识库、检索器、适配器和生成器等。</p>
</blockquote>
<h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><blockquote>
<p>索引环节面临三个主要问题： <strong>块大小跟内容表述完整性问题</strong>：内容块的语义信息受分割方式影响，导致在较长语境中重要信息丢失或被掩盖。 <strong>块相似性搜索准确性问题</strong>：随着数据量增大，检索噪声增多，导致频繁与错误数据匹配，影响检索系统的稳定性和可靠性。（<strong>内容主题跟查询主题有关联性，但是跟查询意图匹配度低</strong>） <strong>参考轨迹不明晰</strong>：检索到的内容块可能来自不同文档，缺乏引用痕迹，可能出现语义相似但主题完全不同的块。（<strong>内容块在不同文档不同条件下可能是矛盾的</strong>）</p>
<h5 id="块优化"><a href="#块优化" class="headerlink" title="块优化"></a>块优化</h5></blockquote>
<ul>
<li><strong>块大小影响</strong>：较大块能提供更多上下文，但噪声多、处理时间长、成本高；较小块噪声少，但可能缺乏上下文。</li>
<li><strong>优化方案</strong>：<ul>
<li><strong>滑动窗口</strong>：将文档分割为多个重叠段落窗口，每个窗口内包含多个句子，通过嵌入向量优化上下文丢失问题，但可能存在句子截断等问题，无法完全处理分割导致的语义完整性破坏结果。</li>
<li><strong>增加元数据</strong>：通过添加页码、文件名、时间戳等元数据，使索引更加精准。</li>
<li><strong>从小到大</strong>：区分检索和合成块，检索时用小块，合成时用大块，以提高检索和合成效率。（段落+章节）</li>
</ul>
</li>
</ul>
<h5 id="结构组织"><a href="#结构组织" class="headerlink" title="结构组织"></a>结构组织</h5><ul>
<li><strong>优化目标</strong>：通过改变索引的组织结构提升检索速度和准确性。</li>
<li><strong>优化方式</strong>：<ul>
<li><strong>多级索引</strong>：通过摘要和文档块创建两个索引，分两步搜索，提升检索质量和响应速度。例如，对于查询“最新发表的RAG论文推荐”，首先通过摘要筛选文献，再根据时间筛选最新论文。</li>
<li><strong>知识图谱</strong>：传统RAG无法有效捕捉实体关系，导致复杂查询无法处理。通过引入知识图谱，提取实体和实体之间的关系，提升复杂查询的精准度。比如“三国演义的主旨是什么”，假设知识图谱中已经建立了书名-主题的关系，则可以快速识别出“主旨”相关的知识并生成合适的回答。</li>
</ul>
</li>
</ul>
<h4 id="检索前"><a href="#检索前" class="headerlink" title="检索前"></a>检索前</h4><h5 id="查询扩展"><a href="#查询扩展" class="headerlink" title="查询扩展"></a>查询扩展</h5><p>查询扩展通过将单一查询转化为多个查询来丰富查询内容，弥补潜在的查询缺失，从而提高答案的相关性。</p>
<ul>
<li><strong>多查询（Multi-Query）</strong>：利用大型语言模型（LLM）扩展原始查询为多个相似查询，并行执行。通过 <strong>RAG-Fusion</strong> 方案，将原始查询扩展为多个查询，进行向量搜索并应用倒数排名融合算法，以获取最佳结果。</li>
<li><strong>子查询</strong>：通过将复杂问题拆解为多个子问题，分别查询并回答。例如，复杂查询“请详细且全面地介绍 RAG”可以拆解为多个子问题，如“RAG 的概念是什么？”、“RAG 有哪些使用场景？”等。</li>
</ul>
<h5 id="查询转换"><a href="#查询转换" class="headerlink" title="查询转换"></a>查询转换</h5><p>查询转换将用户的原始查询改写为新的查询内容进行检索，而非增加查询数量。</p>
<ul>
<li><strong>查询重写</strong>：使用LLM重新表述用户查询，尤其在多轮对话中，将历史信息与新问题一起重新表述。</li>
<li><strong>HYDE</strong>：通过LLM生成假设性答案，再与原始查询共同进行向量检索。假设答案可能包含虚假信息，但能帮助检索相关文档。</li>
<li><strong>Step-Back Prompting</strong>：当原始查询过于复杂或范围广泛时，生成一个抽象层次更高的“后退”问题以增加返回结果，再从中寻找具体答案。对于问题“2000-2024年，哪些人获得了乒乓球奥运会冠军”这个问题因为有时间范围的详细限制，比较难直接解决，可以提出一个后退问题“乒乓球奥运会冠军都有谁”</li>
</ul>
<h5 id="查询构建"><a href="#查询构建" class="headerlink" title="查询构建"></a>查询构建</h5><p>查询构建将自然语言查询转化为机器能理解的格式，尤其在RAG系统中，处理结构化数据（如表格、图形等）成为关键。</p>
<ul>
<li><strong>Text-to-SQL</strong>：将自然语言查询转化为SQL语句进行数据库查询，适用于ChatBI等场景。</li>
<li><strong>Text-to-Cypher</strong>：将自然语言查询转化为图数据库的 Cypher 查询语句，用于在知识图谱等场景中检索结构化关系。</li>
</ul>
<h4 id="检索中"><a href="#检索中" class="headerlink" title="检索中"></a>检索中</h4><p>在检索环节，模块化Rag主要设置了两个优化方向，检索器选择和检索器微调。 <strong>1. 检索器选择（Retriever Selection）：</strong></p>
<ul>
<li><strong>稀疏检索器</strong>：基于<strong>统计方法</strong>将查询和文档转化为稀疏向量，优点是处理大数据集时效率高，但在捕捉复杂语义时不如密集向量有效。<strong>TF-IDF</strong>（词频-逆文档频率，利用分词词频构建稀疏向量，跟文档稀疏向量向乘，得到相似度）和 <strong>BM25</strong> 等。</li>
<li><strong>密集检索器</strong>：使用预训练语言模型为查询和文档生成密集表示，尽管计算和存储成本较高，但能够提供更复杂的语义表示。 <strong>2. 检索器微调（Retriever Fine-tune）：</strong></li>
<li><strong>监督微调</strong>：基于领域标记数据，通过对比学习等方法微调检索模型，特别适用于专业领域或有专有术语的情况。</li>
<li><strong>LM 监督检索器</strong>：利用语言模型生成的结果作为监督信号，在RAG流程中对嵌入模型进行微调，而非直接从数据集中构建微调数据集。</li>
<li><strong>适配器</strong>：一种轻量级模块，在不改变原始模型结构的情况下，增强模型的功能或调整模型行为，以便更好地适配下游任务。比如bert加一层全连接神经网络，进行任务分类判断。</li>
</ul>
<h4 id="检索后"><a href="#检索后" class="headerlink" title="检索后"></a>检索后</h4><blockquote>
<p>在朴素RAG（Retrieval-Augmented Generation）方法中，所有检索到的块直接输入到大语言模型（LLM）生成回答，导致中间内容丢失、噪声过多和上下文长度限制等问题。为了解决这些问题，高级RAG提出了提示压缩和重新排序的方法，模块化RAG将其分为三个模块：重排序、压缩和选择。</p>
</blockquote>
<ol>
<li><strong>重排序（Rerank）</strong>：使用专门的排序模型重新计算检索内容块的相关性得分，考虑查询意图、词汇多重语义、用户历史行为和上下文信息等特征。</li>
<li><strong>压缩（Compression）</strong>：在输入大模型之前，先删除无关内容并突出重要上下文，减少提示长度，降低冗余信息对大模型的干扰。</li>
<li><strong>选择（Selection）</strong>：通过移除无关的文档块来精炼输入，提升推理效率。LLM-Critique是一种有效的选择方法，通过LLM反思机制在生成最终答案前过滤掉相关性不高的文档。</li>
</ol>
<h4 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h4><ul>
<li><strong>生成器微调（Generator Fine-tune）：</strong><ul>
<li><strong>指令微调</strong>：通过微调为大模型提供特定领域的数据，调整输入输出格式以适应特定需求。</li>
<li><strong>强化学习</strong>：通过强化学习让模型输出符合人类、社会偏好，人工标注答案并给予反馈。</li>
<li><strong>Prompt优化</strong>：明确指出RAG系统中的prompt只基于搜索结果回答问题，避免模型添加其他信息。</li>
</ul>
</li>
<li><strong>验证（Verification）：</strong><ul>
<li><strong>知识库验证</strong>：通过外部知识库验证大模型生成的响应。<strong>应用场景</strong>适用于事实性强、可以直接查证的问题（如历史事件、科学原理等），要具有可靠的外部知识库。</li>
<li><strong>基于模型的验证</strong>：使用小型语言模型验证大模型生成的答案是否与检索到的知识一致。 -<strong>小型语言模型的验证：</strong> 1. <strong>输入小型模型</strong>： 小型模型接收以下信息： - <strong>问题</strong>：“光速是多少？” - <strong>检索到的知识</strong>：“光速在真空中的值是xxxxx米每秒。” - <strong>大模型的回答</strong>：“光速是yyyyy米每秒。” 2. <strong>验证过程</strong>： 小型语言模型会执行以下几个步骤： - <strong>语义理解</strong>：小型模型首先理解问题的含义（光速是多少？），并理解大模型的回答（是xxxx米每秒）。 - <strong>事实比对</strong>：然后，它对比从知识库中检索到的内容（光速是yyyy米每秒）与大模型的回答。 - <strong>判断一致性</strong>：小型模型根据语义理解和事实对比，得出结论——大模型的回答“光速是yyyyy米每秒”与检索到的知识一致。 3. <strong>输出结果</strong>： 小型模型返回验证结果：“大模型的回答正确”或者“需要修改”。</li>
</ul>
</li>
</ul>
<h4 id="编排"><a href="#编排" class="headerlink" title="编排"></a>编排</h4><ul>
<li><strong>路由（Routing）</strong>：根据查询的需求和输入的元数据，选择最合适的处理管道或模块。例如，引入多重索引技术后，通过路由机制引导查询至最合适的父级索引。也可以根据不同的query进行分类，路由到不同的prompt生成。</li>
<li><strong>调度（Scheduling）</strong>：调度模块负责控制流程的复杂决策，如递归、反馈等的时机，依据规则判断（例如设定阈值）或从知识图谱中提取信息来决定处理步骤。（知识图谱的结构跟调度策略决定是否需要递归查询更多相关节点，或者是否需要向用户提供额外的信息。）</li>
<li><strong>融合（Fusion）</strong>：融合模块将不同流程的结果合并，并确保输出内容的多维度性和丰富性。例如，利用LLM融合将各分支答案总结后输入LLM，以保证重要内容不丢失；采用互反排名融合将多个检索结果的排名进行合并和加权，提高预测性能。</li>
</ul>
<h2 id="目前的-SOTA"><a href="#目前的-SOTA" class="headerlink" title="目前的 SOTA"></a>目前的 SOTA</h2><p>提示：RAG 没有单一通用榜单，不同任务&#x2F;模态&#x2F;域差异大。实践中可参考以下权威基准与榜单：</p>
<ul>
<li>向量表示（Embeddings）：MTEB Leaderboard <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/mteb/leaderboard">https://huggingface.co/spaces/mteb/leaderboard</a><br>代表选型：BAAI&#x2F;bge-m3、jina-embeddings-v3、OpenAI text-embedding-3-large、Voyage-large-2 等</li>
<li>重排序（Reranking）：Cohere Rerank-3、BAAI&#x2F;bge-reranker-large-v2、Jina Reranker 等<br>评测维度：nDCG@k、MAP、Recall@k，需在本域数据上复现</li>
<li>检索器：密集检索（DPR&#x2F;BGE）、晚交互 ColBERTv2、稀疏检索 BM25&#x2F;SPLADE</li>
<li>端到端 RAG：建议在自建评测集上联合评估“检索质量（覆盖&#x2F;精度）+ 生成可信度（Faithfulness&#x2F;引用正确率）+ 成本&#x2F;时延”。</li>
</ul>
<h2 id="社区框架-amp-技术选型"><a href="#社区框架-amp-技术选型" class="headerlink" title="社区框架&amp;技术选型"></a>社区框架&amp;技术选型</h2><p>面向工程落地按模块选型，优先“可观测性 + 可替换性 + 社区成熟度”。</p>
<h3 id="RAG开源框架"><a href="#RAG开源框架" class="headerlink" title="RAG开源框架"></a>RAG开源框架</h3><ul>
<li>LangChain &#x2F; LangGraph：组件化 + 图式编排，生态丰富，适合从 PoC 到中等复杂度生产。</li>
<li>LlamaIndex：数据接入与索引能力强，提供多种索引&#x2F;检索器与评估工具，适合复杂检索。</li>
<li>Haystack：企业搜索传统优势，检索&#x2F;重排&#x2F;管道稳健，易与 Elastic&#x2F;OpenSearch 集成。</li>
<li>DSPy：提示&#x2F;流程“可编程”与自动化优化，适合复杂策略学习与数据驱动改进。</li>
</ul>
<h3 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h3><ul>
<li>爬取&#x2F;抽取：Playwright&#x2F;Chromium + Trafilatura&#x2F;Readability，保留结构与正文；PDF 用 pdfminer&#x2F;pymupdf。</li>
<li>通用解析：Unstructured.io、Apache Tika，把多格式文档转为干净文本与元数据。</li>
<li>表格&#x2F;结构化：Text-to-SQL 接库；日志&#x2F;指标直接接 Data Lake&#x2F;湖仓。</li>
<li>合规与可追溯：记录来源 URL&#x2F;时间戳&#x2F;版本哈希，便于问责与回溯。</li>
</ul>
<h3 id="数据处理-amp-索引"><a href="#数据处理-amp-索引" class="headerlink" title="数据处理&amp;索引"></a>数据处理&amp;索引</h3><ul>
<li>切分：规则切分（RecursiveCharacterTextSplitter）+ 语义切分（LlamaIndex SemanticSplitter）减少截断；合理 overlap。</li>
<li>元数据：章节&#x2F;标题&#x2F;页码&#x2F;时间&#x2F;作者&#x2F;标签；用于过滤与路由。</li>
<li>多索引：段落向量索引 + 摘要索引（父子索引）+ 关键词稀疏索引；必要时构建知识图谱&#x2F;实体索引。</li>
<li>去重与归并：相似度聚类 + 版本策略，减少冗余。<br>参考：LlamaIndex Semantic Chunking <a target="_blank" rel="noopener" href="https://developers.llamaindex.ai/python/examples/node_parsers/semantic_chunking/">https://developers.llamaindex.ai/python/examples/node_parsers/semantic_chunking/</a></li>
</ul>
<h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><ul>
<li>向量数据库：Milvus（分布式&#x2F;规模化）、Qdrant（Rust&#x2F;性价比高）、Weaviate（混合检索&#x2F;GraphQL）、Pinecone（托管易用）。</li>
<li>检索引擎：Elasticsearch&#x2F;OpenSearch（BM25&#x2F;SPLADE&#x2F;向量混合）。</li>
<li>图数据库：Neo4j&#x2F;TigerGraph（关系&#x2F;多跳检索）。<br>选型要点：容量与扩展性、混合检索能力、延迟&#x2F;吞吐&#x2F;成本、生态与运维。</li>
</ul>
<h3 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h3><ul>
<li>检索器选择：稀疏（BM25&#x2F;SPLADE）vs 密集（BGE&#x2F;DPR）vs 晚交互（ColBERTv2）；多检索器融合常优。</li>
<li>检索器微调：对比学习&#x2F;负采样&#x2F;蒸馏；构造领域难负样本提升判别力。</li>
<li>Top-K 与去冗：调 K 值与去重阈值，权衡覆盖与噪声。</li>
</ul>
<h4 id="请求重写"><a href="#请求重写" class="headerlink" title="请求重写"></a>请求重写</h4><ul>
<li>Multi-Query &#x2F; Sub-Query 分解</li>
<li>RAG-Fusion（互反排名融合，RRF）</li>
<li>HyDE（生成假设答案以增强召回）</li>
<li>Step-Back Prompting（先抽象再落地）</li>
</ul>
<h4 id="数据检索（embedding-amp-reranker）"><a href="#数据检索（embedding-amp-reranker）" class="headerlink" title="数据检索（embedding &amp; reranker）"></a>数据检索（embedding &amp; reranker）</h4><p>向量检索用于高召回，交叉编码器重排用于高精度，两者结合最稳健。如下为常用模型：</p>
<p><strong>向量模型</strong>:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/embeddings">OpenAI Embedding</a></li>
<li><a target="_blank" rel="noopener" href="https://www.voyageai.com/">Voyage Embedding</a></li>
<li><a target="_blank" rel="noopener" href="https://txt.cohere.com/introducing-embed-v3/">Cohere Embedding</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en">Jina Embeddings</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-large-en">BAAI&#x2F;bge-large-en</a></li>
</ul>
<p><strong>Rerank模型</strong>:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://txt.cohere.com/rerank/">CohereAI</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-reranker-base">bge-reranker-base</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-reranker-large">bge-reranker-large</a></li>
</ul>
<h3 id="生成-1"><a href="#生成-1" class="headerlink" title="生成"></a>生成</h3><ul>
<li>提示规范：仅基于给定上下文回答；列出处来源；不知则明示。</li>
<li>结构化输出：JSON&#x2F;Markdown 列表；便于前端&#x2F;下游消费。</li>
<li>证据引用：对每条结论附 DocID&#x2F;页码&#x2F;段落；支持点击回溯。</li>
<li>自我校对：生成后用小模型或 LLM 对“是否被支持&#x2F;是否幻觉”打分，必要时触发重检索。</li>
</ul>
<h4 id="LLM训练推理加速"><a href="#LLM训练推理加速" class="headerlink" title="LLM训练推理加速"></a>LLM训练推理加速</h4><ul>
<li>服务框架：vLLM（PagedAttention、连续批处理、Speculative decoding）<a target="_blank" rel="noopener" href="https://docs.vllm.ai/">https://docs.vllm.ai/</a></li>
<li>GPU 优化：TensorRT-LLM <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM%EF%BC%9B%E9%87%8F%E5%8C%96%EF%BC%88INT8/FP8/4-bit%EF%BC%89%E3%80%81KV">https://github.com/NVIDIA/TensorRT-LLM；量化（INT8/FP8/4-bit）、KV</a> cache 复用</li>
<li>其他：TGI&#x2F;SGLang；提示缓存、相似请求缓存（semantic cache）</li>
</ul>
<h4 id="并发查询－成本、延迟、弹性服务等"><a href="#并发查询－成本、延迟、弹性服务等" class="headerlink" title="并发查询－成本、延迟、弹性服务等"></a>并发查询－成本、延迟、弹性服务等</h4><ul>
<li>并行策略：多查询并发 + 早停 + 部分结果回传；RRF 融合。</li>
<li>缓存：Embedding&#x2F;检索结果&#x2F;重排结果&#x2F;生成结果分层缓存；TTL 与失效策略。</li>
<li>弹性：限流&#x2F;熔断&#x2F;队列优先级；Warm pool；高峰降级路径（仅检索或小模型生成）。</li>
<li>成本：按调用计费监控；压缩 Top-K；Prompt 压缩与引用片段。</li>
</ul>
<h3 id="路由选择"><a href="#路由选择" class="headerlink" title="路由选择"></a>路由选择</h3><ul>
<li>索引路由：按域&#x2F;文档类型&#x2F;时效（新旧知识）选择不同索引&#x2F;检索器。</li>
<li>模型路由：问答&#x2F;摘要&#x2F;多跳&#x2F;代码等意图到不同 Prompt&#x2F;LLM。</li>
<li>策略路由：依据评分（覆盖、相关性、置信度）决定是否重检索&#x2F;重排&#x2F;重生成。</li>
</ul>
<h3 id="准确性测评"><a href="#准确性测评" class="headerlink" title="准确性测评"></a>准确性测评</h3><ul>
<li>离线指标：Context Precision&#x2F;Recall、Context Utilization、Answer Faithfulness、Citation Correctness。</li>
<li>在线指标：用户满意度、采纳率、停留&#x2F;转化、人工复审通过率。</li>
<li>工具与基准：RAGAS <a target="_blank" rel="noopener" href="https://github.com/explodinggradients/ragas%E3%80%81TruLens">https://github.com/explodinggradients/ragas、TruLens</a> <a target="_blank" rel="noopener" href="https://github.com/truera/trulens%E3%80%81ARES%EF%BC%88NAACL">https://github.com/truera/trulens、ARES（NAACL</a> 2024）<a target="_blank" rel="noopener" href="https://aclanthology.org/2024.naacl-long.20/">https://aclanthology.org/2024.naacl-long.20/</a></li>
</ul>
<h2 id="工程化的整体链路"><a href="#工程化的整体链路" class="headerlink" title="工程化的整体链路"></a>工程化的整体链路</h2><ol>
<li>数据采集 → 2) 清洗&#x2F;切分&#x2F;标注 → 3) 多索引构建（向量&#x2F;摘要&#x2F;稀疏&#x2F;图）→ 4) 检索（请求重写&#x2F;多检索器融合）→ 5) 后检索（重排&#x2F;压缩&#x2F;选择）→ 6) 生成（结构化输出+引用）→ 7) 反馈闭环（人工标注&#x2F;在线信号驱动重检索&#x2F;重生成&#x2F;微调）→ 8) 监控与治理（质量&#x2F;时延&#x2F;成本&#x2F;安全）。<br>实践建议：模块可替换、全链路可观测、A&#x2F;B 与回放评测、数据与提示版本化。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flowchart LR</span><br><span class="line">  A[数据采集] --&gt; B[清洗/切分/标注]</span><br><span class="line">  B --&gt; C&#123;多索引构建&#125;</span><br><span class="line">  C --&gt; C1[向量]</span><br><span class="line">  C --&gt; C2[稀疏]</span><br><span class="line">  C --&gt; C3[摘要]</span><br><span class="line">  C --&gt; C4[图谱]</span><br><span class="line">  D[请求重写/子查询] --&gt; E[多检索器融合]</span><br><span class="line">  C1 &amp; C2 &amp; C3 &amp; C4 --&gt; E</span><br><span class="line">  E --&gt; F[后检索: 重排/压缩/选择]</span><br><span class="line">  F --&gt; G[生成: 结构化输出+引用]</span><br><span class="line">  G --&gt; H&#123;验证/自校对&#125;</span><br><span class="line">  H -- 失败 --&gt; D</span><br><span class="line">  H -- 通过 --&gt; I[交付]</span><br><span class="line">  G --&gt; J[监控: 质量/成本/时延]</span><br><span class="line">  J --&gt; K[反馈闭环: 标注/微调/重建索引]</span><br></pre></td></tr></table></figure>


<h3 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h3><h4 id="research-rabbit"><a href="#research-rabbit" class="headerlink" title="research_rabbit"></a>research_rabbit</h4><blockquote>
<p>项目（原 research_rabbit，现已迁移为 local-deep-researcher）：<a target="_blank" rel="noopener" href="https://github.com/langchain-ai/local-deep-researcher">https://github.com/langchain-ai/local-deep-researcher</a></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">  Q[研究主题] --&gt; S[生成搜索 Query]</span><br><span class="line">  S --&gt; R[网页检索]</span><br><span class="line">  R --&gt; SUM[总结与引用]</span><br><span class="line">  SUM --&gt; REFLECT[反思找缺口]</span><br><span class="line">  REFLECT --&gt; S2[生成新 Query]</span><br><span class="line">  S2 --&gt; R2[追加检索]</span><br><span class="line">  R2 --&gt; SUM</span><br><span class="line">  SUM --&gt; OUT[输出 Markdown 总结]</span><br></pre></td></tr></table></figure>

<blockquote>
</blockquote>
<ol>
<li>首先这个项目是用langgraph搭建的，langgraph是基于 Langchain 之上构建的一个扩展库，LangChain 可以方便定义有向无环图(DAG)，但无法定义有环图,LangGraph 可以。</li>
<li>这个项目实现上属于高级RAG的一种<ol>
<li>使用LLM 进行query重写，</li>
<li>使用搜索api实现知识库检索</li>
<li>使用大模型压缩检索检索到的内容，即生成新的summary或者扩展原先的summary</li>
<li>使用大模型进行反思，判断上一步的summary是否欠缺完整性，欠缺则生成相关的query，进行路由处理</li>
<li>路由处理，路由规则是反思轮数达到预设上限(3),输出summary跟参考依据，如果没有重复2,3,4</li>
</ol>
</li>
</ol>
<h3 id="落地案例"><a href="#落地案例" class="headerlink" title="落地案例"></a>落地案例</h3><h4 id="快手"><a href="#快手" class="headerlink" title="快手"></a>快手</h4><ul>
<li>B 端商业化团队“基于 LLM 的智能 RAG 与 Agent 平台”实践，覆盖检索重排、任务编排、知识治理与可观测性。</li>
<li>参考：知乎专栏（2024-09）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/721878813">https://zhuanlan.zhihu.com/p/721878813</a></li>
</ul>
<h4 id="智谱"><a href="#智谱" class="headerlink" title="智谱"></a>智谱</h4><ul>
<li>“清语知识库”：基于 RAG 的企业知识问答平台，配合 AICO 编排引擎，支持知识摄取、检索、重排与多场景问答。</li>
<li>参考：昇腾社区解决方案页（2024-11）<a target="_blank" rel="noopener" href="https://www.hiascend.com/marketplace/solution/detail/2389">https://www.hiascend.com/marketplace/solution/detail/2389</a></li>
</ul>
<h4 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a>阿里</h4><ul>
<li>阿里云 OpenSearch&#x2F;搜索开发工作台：提供“一键式”AI 语义搜索与 RAG 链路搭建、最佳实践模板与可视化配置。<ul>
<li>InfoQ｜解读阿里云搜索开发工作台如何快速搭建 AI 语义搜索及 RAG 链路（2024-07）：<a target="_blank" rel="noopener" href="https://xie.infoq.cn/article/57f04ecf53863c39daced1e7d">https://xie.infoq.cn/article/57f04ecf53863c39daced1e7d</a></li>
<li>云栖大会实录｜大模型驱动、开源融合的 AI 搜索产品发布（2024-10）：<a target="_blank" rel="noopener" href="https://xie.infoq.cn/article/183b9f3b2929c8a440932240a">https://xie.infoq.cn/article/183b9f3b2929c8a440932240a</a></li>
</ul>
</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="知识库搭建"><a href="#知识库搭建" class="headerlink" title="知识库搭建"></a>知识库搭建</h3><ul>
<li>数据治理：来源可信（权威&#x2F;一手）、去重与版本化、标注时效（生效&#x2F;失效时间）。</li>
<li>切片策略：按语义&#x2F;结构切分并设置适度 overlap；面向检索与面向生成的块分离（小块检索&#x2F;大块合成）。</li>
<li>多索引：向量 + 稀疏 + 摘要 + 实体&#x2F;关系（图）；结合路由与过滤。</li>
<li>元数据与权限：组织、业务、保密级别、有效期；用于访问控制与路由。</li>
<li>更新机制：增量构建、冷&#x2F;热合并、回收旧版本；Embedding&#x2F;索引定期重建。</li>
<li>监控与回溯：每条回答记录引用的文档与片段位置，支持质检与审计。</li>
</ul>
<h2 id="新型RAG范式原理与实现"><a href="#新型RAG范式原理与实现" class="headerlink" title="新型RAG范式原理与实现"></a>新型RAG范式原理与实现</h2><h3 id="自纠错RAG：C-RAG"><a href="#自纠错RAG：C-RAG" class="headerlink" title="自纠错RAG：C-RAG"></a>自纠错RAG：C-RAG</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.15884">https://arxiv.org/abs/2401.15884</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/HuskyInSalt/CRAG">https://github.com/HuskyInSalt/CRAG</a></p>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-02.png"></p>
<h3 id="自省式RAG：Self-RAG"><a href="#自省式RAG：Self-RAG" class="headerlink" title="自省式RAG：Self-RAG"></a>自省式RAG：Self-RAG</h3><p><a target="_blank" rel="noopener" href="https://github.com/AkariAsai/self-rag">https://github.com/AkariAsai/self-rag</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.11511">https://arxiv.org/abs/2310.11511</a></p>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-03.png"></p>
<h3 id="检索树RAG：RAPTOR"><a href="#检索树RAG：RAPTOR" class="headerlink" title="检索树RAG：RAPTOR"></a>检索树RAG：RAPTOR</h3><p><a target="_blank" rel="noopener" href="https://github.com/parthsarthi03/raptor">https://github.com/parthsarthi03/raptor</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.18059">https://arxiv.org/abs/2401.18059</a></p>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-04.png"></p>
<h3 id="MemoRAG"><a href="#MemoRAG" class="headerlink" title="MemoRAG"></a>MemoRAG</h3><p><a target="_blank" rel="noopener" href="https://github.com/qhjqhj00/MemoRAG">https://github.com/qhjqhj00/MemoRAG</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.05591">https://arxiv.org/abs/2409.05591</a></p>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-05.png"></p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45783724/article/details/140409288">论文分享|RAG理论-第一篇-概述_rag原理文献-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/3210586096?utm_medium=social&utm_psn=1852642205329674240&utm_source=wechat_session">https://zhuanlan.zhihu.com/p/3210586096?utm_medium&#x3D;social&amp;utm_psn&#x3D;1852642205329674240&amp;utm_source&#x3D;wechat_session</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45783724/article/details/140317428">项目分享|RAG-Retrieval库实现基于LLM偏好监督RAG检索器微调-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://hyper.ai/cn/wiki/29013">HyperAI超神经</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45783724/article/details/140626610">论文分享|SIGIR2024最佳论文’清华|密集检索的Scaling Laws_scaling laws for dense retrieval-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ei421a7Kc/?buvid=Y74F527BC8DD923B4266ACE1D6F69D32DA4A&from_spmid=search.search-result.0.0&is_story_h5=false&mid=+fkR0uFdfHhzd4hD1T16nw==&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=8EE5AAC3-71B2-444B-B686-C6A0F5F01937&share_source=WEIXIN&share_source=weixin&share_tag=s_i&timestamp=1734914851&unique_k=ECcQlvm&up_id=3494363130759443&vd_source=1375edbf0b3a47842f71cdbe22e60304">大模型RAG企业项目实战：手把手带你搭建一套完整的RAG系统，原理讲解+代码解析，草履虫都能学明白！LLM大模型_RAG_大模型微调_多模态_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://sbert.net/">SentenceTransformers Documentation — Sentence Transformers documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://luxiangdong.com/2024/11/14/rag2024/">RAG的2024—随需而变，从狂热到理性</a></p>
<p><a target="_blank" rel="noopener" href="https://www.163.com/dy/article/JJI2VQ1L051193U6.html">16种新型RAG最新进展</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.whispergenie.com">Scharfsinnig</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.whispergenie.com/post/64926.html">https://blog.whispergenie.com/post/64926.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.whispergenie.com" target="_blank">SCHARFSINNIGの博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/RAG/">RAG</a><a class="post-meta__tags" href="/tags/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90/">检索增强生成</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/">信息检索</a></div><div class="post_share"><div class="social-share" data-image="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-01.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-card"><div class="reward-heading"><i class="fas fa-leaf"></i><span>随缘一杯茶</span></div><p class="reward-desc">若这些文字曾在某个时刻与你相和，如同一盏清茶带来一分安定与回甘；你若有心，且请我饮茶一杯，让创作在茶香里慢慢生长——随缘赞赏，心安即可。</p><ul class="reward-qr-grid"><li class="reward-pay-item"><a href="/images/qr-code/wechat-qrcode.png" target="_blank" aria-label="微信"><img src="/images/qr-code/wechat-qrcode.png" alt="微信" loading="lazy" width="200" height="200"/></a><span class="reward-pay-label">微信</span></li><li class="reward-pay-item"><a href="/images/qr-code/alipay-qrcode.png" target="_blank" aria-label="支付宝"><img src="/images/qr-code/alipay-qrcode.png" alt="支付宝" loading="lazy" width="200" height="200"/></a><span class="reward-pay-label">支付宝</span></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/post/62629.html"><img class="next-cover" src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/AI-Agent-%E6%BC%94%E5%8F%98%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/image-01.png" onerror="onerror=null;src='/images/ui/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">AI Agent：演变、架构与实际应用</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatars/avator.png" onerror="this.onerror=null;this.src='/img/index/avator.png'" alt="avatar"/></div><div class="author-info__name">Scharfsinnig</div><div class="author-info__description">博学、慎思、明辨、笃行 | O ever youthful.O ever weeping.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/scharfsinnig"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/scharfsinnig" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:admin@whispergenie.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90"><span class="toc-number">1.</span> <span class="toc-text">检索增强生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6%E8%84%89%E7%BB%9C"><span class="toc-number">1.2.</span> <span class="toc-text">学术研究脉络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E5%B1%B1%E8%AE%BA%E6%96%87"><span class="toc-number">1.2.1.</span> <span class="toc-text">开山论文</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Retrieval-augmented-generation-for-knowledge-intensive-NLP-tasks"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Retrieval-augmented generation for knowledge-intensive NLP tasks</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E7%9A%84%E8%83%8C%E6%99%AF"><span class="toc-number">1.2.1.1.1.</span> <span class="toc-text">问题的背景</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.1.1.2.</span> <span class="toc-text">提出的方法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.2.1.1.3.</span> <span class="toc-text">结论</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%BC%E8%BF%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">综述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Retrieval-Augmented-Generation-for-Large-Language-Models-A-Survey"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Retrieval-Augmented Generation for Large Language Models: A Survey</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Retrieval-Augmented-Generation-A-Survey"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Graph Retrieval-Augmented Generation: A Survey</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#A-Comprehensive-Survey-of-Retrieval-Augmented-Generation-RAG-Evolution-Current-Landscape-and-Future-Directions"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Agentic-RAG"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">Agentic RAG</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAG%E8%8C%83%E5%BC%8F%EF%BC%88%E6%B5%81%E7%A8%8B-amp-%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">RAG范式（流程&amp;整体结构）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E5%8D%95RAG%EF%BC%88Naive-RAG%EF%BC%89"><span class="toc-number">1.3.1.</span> <span class="toc-text">简单RAG（Naive RAG）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E8%A1%A8%E6%80%A7%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">代表性工作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7RAG%EF%BC%88Advanced-RAG%EF%BC%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">高级RAG（Advanced RAG）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%A3%80%E7%B4%A2%E8%BF%87%E7%A8%8B%EF%BC%88Pre-Retrieval-Process%EF%BC%89"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">预检索过程（Pre-Retrieval Process）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E7%B4%A2%E5%BC%95"><span class="toc-number">1.3.2.1.1.</span> <span class="toc-text">建立索引</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%9F%A5%E8%AF%A2"><span class="toc-number">1.3.2.1.2.</span> <span class="toc-text">处理查询</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E6%A3%80%E7%B4%A2%E8%BF%87%E7%A8%8B%EF%BC%88Post-Retrieval-Process%EF%BC%89"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">后检索过程（Post-Retrieval Process）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E5%8E%8B%E7%BC%A9"><span class="toc-number">1.3.2.2.1.</span> <span class="toc-text">提示压缩</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%87%8D%E6%8E%92%E5%BA%8F"><span class="toc-number">1.3.2.2.2.</span> <span class="toc-text">重排序</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E5%8C%96RAG%EF%BC%88Modular-RAG%EF%BC%89"><span class="toc-number">1.3.3.</span> <span class="toc-text">模块化RAG（Modular RAG）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">索引</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9D%97%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.3.1.1.</span> <span class="toc-text">块优化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%93%E6%9E%84%E7%BB%84%E7%BB%87"><span class="toc-number">1.3.3.1.2.</span> <span class="toc-text">结构组织</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E5%89%8D"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">检索前</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E6%89%A9%E5%B1%95"><span class="toc-number">1.3.3.2.1.</span> <span class="toc-text">查询扩展</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.3.3.2.2.</span> <span class="toc-text">查询转换</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E6%9E%84%E5%BB%BA"><span class="toc-number">1.3.3.2.3.</span> <span class="toc-text">查询构建</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E4%B8%AD"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">检索中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E5%90%8E"><span class="toc-number">1.3.3.4.</span> <span class="toc-text">检索后</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E6%88%90"><span class="toc-number">1.3.3.5.</span> <span class="toc-text">生成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E6%8E%92"><span class="toc-number">1.3.3.6.</span> <span class="toc-text">编排</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%89%8D%E7%9A%84-SOTA"><span class="toc-number">1.4.</span> <span class="toc-text">目前的 SOTA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A4%BE%E5%8C%BA%E6%A1%86%E6%9E%B6-amp-%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B"><span class="toc-number">1.5.</span> <span class="toc-text">社区框架&amp;技术选型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RAG%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6"><span class="toc-number">1.5.1.</span> <span class="toc-text">RAG开源框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96"><span class="toc-number">1.5.2.</span> <span class="toc-text">数据获取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-amp-%E7%B4%A2%E5%BC%95"><span class="toc-number">1.5.3.</span> <span class="toc-text">数据处理&amp;索引</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="toc-number">1.5.4.</span> <span class="toc-text">数据存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2"><span class="toc-number">1.5.5.</span> <span class="toc-text">检索</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%B7%E6%B1%82%E9%87%8D%E5%86%99"><span class="toc-number">1.5.5.1.</span> <span class="toc-text">请求重写</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2%EF%BC%88embedding-amp-reranker%EF%BC%89"><span class="toc-number">1.5.5.2.</span> <span class="toc-text">数据检索（embedding &amp; reranker）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90-1"><span class="toc-number">1.5.6.</span> <span class="toc-text">生成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F"><span class="toc-number">1.5.6.1.</span> <span class="toc-text">LLM训练推理加速</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B6%E5%8F%91%E6%9F%A5%E8%AF%A2%EF%BC%8D%E6%88%90%E6%9C%AC%E3%80%81%E5%BB%B6%E8%BF%9F%E3%80%81%E5%BC%B9%E6%80%A7%E6%9C%8D%E5%8A%A1%E7%AD%89"><span class="toc-number">1.5.6.2.</span> <span class="toc-text">并发查询－成本、延迟、弹性服务等</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%AF%E7%94%B1%E9%80%89%E6%8B%A9"><span class="toc-number">1.5.7.</span> <span class="toc-text">路由选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E6%80%A7%E6%B5%8B%E8%AF%84"><span class="toc-number">1.5.8.</span> <span class="toc-text">准确性测评</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B7%A5%E7%A8%8B%E5%8C%96%E7%9A%84%E6%95%B4%E4%BD%93%E9%93%BE%E8%B7%AF"><span class="toc-number">1.6.</span> <span class="toc-text">工程化的整体链路</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B"><span class="toc-number">1.6.1.</span> <span class="toc-text">应用案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#research-rabbit"><span class="toc-number">1.6.1.1.</span> <span class="toc-text">research_rabbit</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%90%BD%E5%9C%B0%E6%A1%88%E4%BE%8B"><span class="toc-number">1.6.2.</span> <span class="toc-text">落地案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BF%AB%E6%89%8B"><span class="toc-number">1.6.2.1.</span> <span class="toc-text">快手</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%99%BA%E8%B0%B1"><span class="toc-number">1.6.2.2.</span> <span class="toc-text">智谱</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%BF%E9%87%8C"><span class="toc-number">1.6.2.3.</span> <span class="toc-text">阿里</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.7.</span> <span class="toc-text">总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%BA%93%E6%90%AD%E5%BB%BA"><span class="toc-number">1.7.1.</span> <span class="toc-text">知识库搭建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B0%E5%9E%8BRAG%E8%8C%83%E5%BC%8F%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.8.</span> <span class="toc-text">新型RAG范式原理与实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%BA%A0%E9%94%99RAG%EF%BC%9AC-RAG"><span class="toc-number">1.8.1.</span> <span class="toc-text">自纠错RAG：C-RAG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%9C%81%E5%BC%8FRAG%EF%BC%9ASelf-RAG"><span class="toc-number">1.8.2.</span> <span class="toc-text">自省式RAG：Self-RAG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E6%A0%91RAG%EF%BC%9ARAPTOR"><span class="toc-number">1.8.3.</span> <span class="toc-text">检索树RAG：RAPTOR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MemoRAG"><span class="toc-number">1.8.4.</span> <span class="toc-text">MemoRAG</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3"><span class="toc-number">1.9.</span> <span class="toc-text">参考文档</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/64926.html" title="RAG系统性调研：范式、方法与工程化路径"><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-01.png" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="RAG系统性调研：范式、方法与工程化路径"/></a><div class="content"><a class="title" href="/post/64926.html" title="RAG系统性调研：范式、方法与工程化路径">RAG系统性调研：范式、方法与工程化路径</a><time datetime="2025-11-10T09:03:56.000Z" title="发表于 2025-11-10 17:03:56">2025-11-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/62629.html" title="AI Agent：演变、架构与实际应用"><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/AI-Agent-%E6%BC%94%E5%8F%98%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/image-01.png" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="AI Agent：演变、架构与实际应用"/></a><div class="content"><a class="title" href="/post/62629.html" title="AI Agent：演变、架构与实际应用">AI Agent：演变、架构与实际应用</a><time datetime="2025-11-10T09:03:56.000Z" title="发表于 2025-11-10 17:03:56">2025-11-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/33348.html" title="中医第一篇：记录关于&quot;中医&quot;的一些感悟"><img src="/images/blogs/%E5%8F%A4%E5%85%B8%E6%99%BA%E6%85%A7/%E4%B8%AD%E5%8C%BB%E7%AC%AC%E4%B8%80%E7%AF%87%EF%BC%9A%E8%AE%B0%E5%BD%95%E5%85%B3%E4%BA%8E%E2%80%9C%E4%B8%AD%E5%8C%BB%E2%80%9D%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%82%9F/%E5%8F%A4%E4%B8%AD%E5%8C%BB%E6%82%9F%E4%B9%A6%E7%B1%8D.png" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="中医第一篇：记录关于&quot;中医&quot;的一些感悟"/></a><div class="content"><a class="title" href="/post/33348.html" title="中医第一篇：记录关于&quot;中医&quot;的一些感悟">中医第一篇：记录关于&quot;中医&quot;的一些感悟</a><time datetime="2025-11-09T11:27:39.000Z" title="发表于 2025-11-09 19:27:39">2025-11-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/ai-maxim-ronghui-yuxin.html" title="以前的“学好数理化，走遍天下都不怕”，在当下的AI浪潮中，会进化成什么？"><img src="/images/covers/ai_01.jpg" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="以前的“学好数理化，走遍天下都不怕”，在当下的AI浪潮中，会进化成什么？"/></a><div class="content"><a class="title" href="/post/ai-maxim-ronghui-yuxin.html" title="以前的“学好数理化，走遍天下都不怕”，在当下的AI浪潮中，会进化成什么？">以前的“学好数理化，走遍天下都不怕”，在当下的AI浪潮中，会进化成什么？</a><time datetime="2025-10-27T13:47:00.000Z" title="发表于 2025-10-27 21:47:00">2025-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/29827.html" title="新名词学习：差序格局"><img src="/images/blogs/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/%E5%B7%AE%E5%BA%8F%E6%A0%BC%E5%B1%80/%E5%B7%AE%E5%BA%8F%E6%A0%BC%E5%B1%80.png" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="新名词学习：差序格局"/></a><div class="content"><a class="title" href="/post/29827.html" title="新名词学习：差序格局">新名词学习：差序格局</a><time datetime="2025-10-17T13:49:00.000Z" title="发表于 2025-10-17 21:49:00">2025-10-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By Scharfsinnig</div><div class="footer_custom_text">嗨~不肉，welcome to my blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><!-- load lazyload before main.js so LazyLoad class exists when main initializes--><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0.27/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@9.1.2/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadGiscus () {
  let nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'light'

  const config = Object.assign({
    src: 'https://giscus.app/client.js',
    'data-repo': 'Scharfsinnig/scharfsinnig.github.io',
    'data-repo-id': 'R_kgDOHp7n3A',
    'data-category-id': 'DIC_kwDOHp7n3M4Cvyzj',
    'data-mapping': 'pathname',
    'data-theme': nowTheme,
    'data-reactions-enabled': '1',
    crossorigin: 'anonymous',
    async: true
  },{"data-lang":"zh-CN","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-category":"Announcements"})

  let ele = document.createElement('script')
  for (let key in config) {
    ele.setAttribute(key, config[key])
  }
  document.getElementById('giscus-wrap').insertAdjacentElement('afterbegin',ele)
}

function changeGiscusTheme () {
  const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'light'

  function sendMessage(message) {
    const iframe = document.querySelector('iframe.giscus-frame');
    if (!iframe) return;
    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
  }

  sendMessage({
    setConfig: {
      theme: theme
    }
  });
}

if ('Giscus' === 'Giscus' || !false) {
  if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
  else loadGiscus()
} else {
  function loadOtherComment () {
    loadGiscus()
  }
}</script></div><div id="player-flyout" aria-hidden="true"><div id="player-mount"></div></div><script src="/js/custom/player-flyout.js"></script><script src="/js/custom/now-playing.js"></script><script src="/js/custom/flink-enhance.js"></script><script src="/js/custom/mermaid-modal-simple.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="102,126,234" opacity="0.3" zIndex="-1" count="66" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="💖,✨,🌟,💫,🎉,🚀" data-fontsize="18px" data-random="true" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["link[rel=\"canonical\"]","meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script src="/js/tech-effects.js"></script><script>(function(){
  function swapLazyImages(){
    try{
      var imgs = document.querySelectorAll('img[data-lazy-src]');
      imgs.forEach(function(img){
        var real = img.getAttribute('data-lazy-src');
        if (real && img.src !== real) {
          img.src = real;
          img.removeAttribute('data-lazy-src');
        }
      });
    } catch(e){}
  }
  function schedule(){
    swapLazyImages();
    var tries = 0;
    var timer = setInterval(function(){
      tries++;
      swapLazyImages();
      if (tries >= 10 || !document.querySelector('img[data-lazy-src]')) {
        clearInterval(timer);
      }
    }, 500);
  }
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', schedule);
  } else {
    schedule();
  }
  window.addEventListener('load', swapLazyImages);
})();</script></div></body></html>