<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Context Engineering学习笔记：理解 Context Rot 与 StreamBench | SCHARFSINNIGの博客</title><meta name="keywords" content="AI Agent,Context Engineering,Context Rot,LLM,长上下文"><meta name="author" content="Scharfsinnig"><meta name="copyright" content="Scharfsinnig"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="深入探讨 Context Rot 现象的技术原理与成因，系统性介绍 StreamBench 论文如何通过流式记忆管理解决 Context Memory 的核心挑战。">
<meta property="og:type" content="article">
<meta property="og:title" content="Context Engineering学习笔记：理解 Context Rot 与 StreamBench">
<meta property="og:url" content="https://blog.whispergenie.com/post/62630.html">
<meta property="og:site_name" content="SCHARFSINNIGの博客">
<meta property="og:description" content="深入探讨 Context Rot 现象的技术原理与成因，系统性介绍 StreamBench 论文如何通过流式记忆管理解决 Context Memory 的核心挑战。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.whispergenie.com/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/context%20engineering%E5%AF%BC%E8%88%AA%E5%A4%B4.png">
<meta property="article:published_time" content="2025-12-17T13:56:00.000Z">
<meta property="article:modified_time" content="2026-01-08T04:33:09.910Z">
<meta property="article:author" content="Scharfsinnig">
<meta property="article:tag" content="AI Agent">
<meta property="article:tag" content="Context Engineering">
<meta property="article:tag" content="Context Rot">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="长上下文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.whispergenie.com/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/context%20engineering%E5%AF%BC%E8%88%AA%E5%A4%B4.png"><link rel="shortcut icon" href="/img/blog-icon-02.png"><link rel="canonical" href="https://blog.whispergenie.com/post/62630"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//www.clarity.ms"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0.27/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d31df79654e408080cd6cc34a11919ee";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script><script>(function(c,l,a,r,i,t,y){
    c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
    t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
    y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
})(window, document, "clarity", "script", "tp16hgqun6");
</script><script defer="defer" src="https://cloud.umami.is/script.js" data-website-id="1b2c08f8-fc2f-47ca-9d18-77a59492990e"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":100,"languages":{"author":"作者: Scharfsinnig","link":"链接: ","source":"来源: SCHARFSINNIGの博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2.1.2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Context Engineering学习笔记：理解 Context Rot 与 StreamBench',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-08 12:33:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#1a1a2e')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><script>var meting_api="https://api.injahow.cn/meting/?server=:server&type=:type&id=:id&r=:r";</script><script defer data-domain="blog.whispergenie.com" src="https://plausible.io/js/script.js"></script><link rel="stylesheet" href="/css/mermaid-modal.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatars/new_avator.png" onerror="this.onerror=null;this.src='/img/index/avator.png'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">48</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 充电站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://ocw.mit.edu/"><i class="fa-fw fa fa-university"></i><span> MIT 公开课</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.youtube.com/"><i class="fa-fw fab fa-youtube"></i><span> YouTube</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://see.stanford.edu/"><i class="fa-fw fa fa-university"></i><span> 斯坦福公开课</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://huggingface.co/"><i class="fa-fw fas fa-robot"></i><span> Hugging Face</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://modelscope.cn/"><i class="fa-fw fas fa-cubes"></i><span> ModelScope</span></a></li><li><a class="site-page child" href="/bloggers/"><i class="fa-fw fa fa-rss"></i><span> 优秀博主博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://twitter.com/"><i class="fa-fw fab fa-twitter"></i><span> X/Twitter</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言信箱</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于博主</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-tools"></i><span> 工具</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/context%20engineering%E5%AF%BC%E8%88%AA%E5%A4%B4.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">SCHARFSINNIGの博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 充电站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://ocw.mit.edu/"><i class="fa-fw fa fa-university"></i><span> MIT 公开课</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.youtube.com/"><i class="fa-fw fab fa-youtube"></i><span> YouTube</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://see.stanford.edu/"><i class="fa-fw fa fa-university"></i><span> 斯坦福公开课</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://huggingface.co/"><i class="fa-fw fas fa-robot"></i><span> Hugging Face</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://modelscope.cn/"><i class="fa-fw fas fa-cubes"></i><span> ModelScope</span></a></li><li><a class="site-page child" href="/bloggers/"><i class="fa-fw fa fa-rss"></i><span> 优秀博主博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://twitter.com/"><i class="fa-fw fab fa-twitter"></i><span> X/Twitter</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言信箱</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于博主</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-tools"></i><span> 工具</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Context Engineering学习笔记：理解 Context Rot 与 StreamBench</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-17T13:56:00.000Z" title="发表于 2025-12-17 21:56:00">2025-12-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-08T04:33:09.910Z" title="更新于 2026-01-08 12:33:09">2026-01-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/">技术洞察</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/context%20engineering%E5%AF%BC%E8%88%AA%E5%A4%B4.png" alt="Context Engineering 导航头"></p>
<p>随着大型语言模型（LLM）的上下文窗口从最初的 4,096 tokens 扩展到如今的百万级别，一个关键问题逐渐浮出水面：<strong>模型真的能够有效利用这些超长上下文吗？</strong></p>
<p>答案是令人担忧的。研究表明，随着输入长度的增加，LLM 的性能会出现显著且非均匀的下降——这一现象被称为 <strong>Context Rot（上下文腐烂）</strong>。</p>
<p>本文将从以下三个层面进行系统性探讨：</p>
<ol>
<li><strong>Context Rot 的技术原理</strong>：深入分析其产生的根本原因</li>
<li><strong>Context Engineering 的体系框架</strong>：理解上下文工程的核心模块</li>
<li><strong>StreamBench 的定位与贡献</strong>：明确其在 Context Memory 管理中解决的具体问题</li>
</ol>
<hr>
<h2 id="第一部分：Context-Rot-深度剖析"><a href="#第一部分：Context-Rot-深度剖析" class="headerlink" title="第一部分：Context Rot 深度剖析"></a>第一部分：Context Rot 深度剖析</h2><h3 id="1-1-什么是-Context-Rot？"><a href="#1-1-什么是-Context-Rot？" class="headerlink" title="1.1 什么是 Context Rot？"></a>1.1 什么是 Context Rot？</h3><p>Context Rot 是指 LLM 在处理长上下文时，性能随着输入 token 数量增加而逐渐退化的现象。这不是简单的”信息太多处理不过来”，而是一种<strong>结构性的能力衰减</strong>。</p>
<p>Chroma Research 在 2025 年 7 月发布的技术报告中，对 18 个主流 LLM（包括 GPT-4.1、Claude 4、Gemini 2.5 和 Qwen3）进行了系统性评估，核心发现包括：</p>
<ul>
<li><strong>非均匀性能衰减</strong>：模型处理第 10,000 个 token 的可靠性远低于第 100 个 token</li>
<li><strong>简单任务同样受影响</strong>：即使是”重复单词”这样的简单任务也会出现显著退化</li>
<li><strong>现有基准的盲区</strong>：传统的 Needle in a Haystack（NIAH）测试过于简单，无法反映真实场景</li>
</ul>
<h3 id="1-2-Context-Rot-的根本原因"><a href="#1-2-Context-Rot-的根本原因" class="headerlink" title="1.2 Context Rot 的根本原因"></a>1.2 Context Rot 的根本原因</h3><h4 id="1-2-1-Transformer-注意力机制的固有约束"><a href="#1-2-1-Transformer-注意力机制的固有约束" class="headerlink" title="1.2.1 Transformer 注意力机制的固有约束"></a>1.2.1 Transformer 注意力机制的固有约束</h4><p>Context Rot 的根本原因在于 Transformer 架构的<strong>自注意力机制（Self-Attention）</strong>。</p>
<p><strong>计算复杂度问题</strong>：自注意力的计算复杂度为 **O(n²)**，其中 n 是序列长度。这意味着：</p>
<ul>
<li>当上下文从 1K 增加到 100K tokens 时，注意力计算量增加 10,000 倍</li>
<li>每个 token 都需要与所有其他 token 计算注意力权重</li>
</ul>
<p><strong>注意力稀释效应</strong>：Anthropic 在其博客中精准地描述了这一现象：</p>
<blockquote>
<p>Context must be treated as a finite resource with diminishing marginal returns. Like humans, who have limited working memory capacity, LLMs have an “attention budget” that they draw on when parsing large volumes of context. Every new token introduced depletes this budget by some amount.</p>
</blockquote>
<p>这意味着：</p>
<ul>
<li>模型拥有一个有限的”注意力预算”</li>
<li>每增加一个 token，都会稀释对其他 token 的注意力</li>
<li>关键信息可能被淹没在海量的无关信息中</li>
</ul>
<h4 id="1-2-2-位置编码的局限性"><a href="#1-2-2-位置编码的局限性" class="headerlink" title="1.2.2 位置编码的局限性"></a>1.2.2 位置编码的局限性</h4><p>现代 LLM 使用各种位置编码方案（如 RoPE、ALiBi）来表示 token 的位置信息，但这些方案存在固有局限：</p>
<p><strong>外推能力不足</strong>：模型在训练时使用的上下文长度通常远小于推理时的最大长度。虽然可以通过技术手段（如位置插值）扩展上下文窗口，但这种扩展往往伴随性能损失。</p>
<p><strong>位置衰减效应</strong>：研究表明，模型对不同位置的 token 关注度不均匀：</p>
<ul>
<li>开头和结尾的 token 通常获得更多注意力（”首位效应”和”近因效应”）</li>
<li>中间位置的 token 容易被忽视（”Lost in the Middle” 现象）</li>
</ul>
<h4 id="1-2-3-信息检索-vs-信息整合的差异"><a href="#1-2-3-信息检索-vs-信息整合的差异" class="headerlink" title="1.2.3 信息检索 vs 信息整合的差异"></a>1.2.3 信息检索 vs 信息整合的差异</h4><p>Context Rot 在不同任务类型上表现不同：</p>
<table>
<thead>
<tr>
<th>任务类型</th>
<th>Context Rot 程度</th>
<th>原因分析</th>
</tr>
</thead>
<tbody><tr>
<td>简单检索（如 NIAH）</td>
<td>较轻</td>
<td>只需定位单一信息</td>
</tr>
<tr>
<td>多跳推理</td>
<td>严重</td>
<td>需要整合多个分散信息</td>
</tr>
<tr>
<td>信息综合</td>
<td>严重</td>
<td>需要理解全局结构</td>
</tr>
<tr>
<td>精确复现</td>
<td>中等</td>
<td>受位置编码和注意力分布影响</td>
</tr>
</tbody></table>
<h3 id="1-3-Chroma-的系统性实验"><a href="#1-3-Chroma-的系统性实验" class="headerlink" title="1.3 Chroma 的系统性实验"></a>1.3 Chroma 的系统性实验</h3><p>Chroma 团队设计了一系列精心构造的实验来量化 Context Rot：</p>
<h4 id="实验一：Needle-Question-语义相似度"><a href="#实验一：Needle-Question-语义相似度" class="headerlink" title="实验一：Needle-Question 语义相似度"></a>实验一：Needle-Question 语义相似度</h4><p><strong>实验设计</strong>：改变”针”（目标信息）与”问题”之间的语义相似度</p>
<p><strong>核心发现</strong>：</p>
<ul>
<li>当 needle 与 question 语义相似度高时，模型表现较好</li>
<li>当语义差距增大时，性能下降更为陡峭</li>
<li>这反映了真实场景中用户查询与相关信息往往存在语义鸿沟</li>
</ul>
<p><strong>技术解释</strong>：模型依赖语义相似性来引导注意力分配。当查询与目标信息语义差距大时，模型难以建立有效的注意力连接。</p>
<h4 id="实验二：干扰项的非均匀影响"><a href="#实验二：干扰项的非均匀影响" class="headerlink" title="实验二：干扰项的非均匀影响"></a>实验二：干扰项的非均匀影响</h4><p><strong>实验设计</strong>：在 haystack 中添加不同数量和类型的干扰项</p>
<p><strong>核心发现</strong>：</p>
<ul>
<li>单个干扰项即可导致显著性能下降</li>
<li>不同干扰项的影响<strong>高度不均匀</strong>——某些干扰项特别容易误导模型</li>
<li>干扰项的语义相似度和位置都会影响其干扰效果</li>
</ul>
<p><strong>技术解释</strong>：干扰项与目标信息竞争模型的注意力资源。当干扰项与查询语义相似时，可能”劫持”本应指向目标信息的注意力。</p>
<h4 id="实验三：Haystack-结构的反直觉影响"><a href="#实验三：Haystack-结构的反直觉影响" class="headerlink" title="实验三：Haystack 结构的反直觉影响"></a>实验三：Haystack 结构的反直觉影响</h4><p><strong>实验设计</strong>：比较逻辑连贯的 haystack 与随机打乱的 haystack</p>
<p><strong>核心发现</strong>（反直觉）：</p>
<ul>
<li><strong>随机打乱的 haystack 反而提升了模型性能</strong></li>
<li>逻辑连贯的文本更容易导致模型”迷失”</li>
</ul>
<p><strong>技术解释</strong>：</p>
<ul>
<li>连贯文本形成的语义流可能干扰模型对目标信息的定位</li>
<li>随机打乱破坏了这种”语义惯性”，使目标信息更容易”跳出”</li>
<li>这暗示模型的注意力机制可能过度依赖局部结构模式</li>
</ul>
<h4 id="实验四：重复单词任务"><a href="#实验四：重复单词任务" class="headerlink" title="实验四：重复单词任务"></a>实验四：重复单词任务</h4><p><strong>实验设计</strong>：让模型重复一系列包含一个独特单词的词列表</p>
<p><strong>核心发现</strong>：</p>
<ul>
<li>随着列表长度增加，准确率显著下降</li>
<li>模型出现<strong>过度生成</strong>（生成多余内容）和<strong>幻觉</strong>（生成不存在的单词）</li>
<li>不同模型家族表现出截然不同的失败模式</li>
</ul>
<h3 id="1-4-不同模型的失败模式"><a href="#1-4-不同模型的失败模式" class="headerlink" title="1.4 不同模型的失败模式"></a>1.4 不同模型的失败模式</h3><p>Chroma 的研究揭示了不同模型家族在 Context Rot 下的行为差异：</p>
<table>
<thead>
<tr>
<th>模型家族</th>
<th>典型失败模式</th>
<th>特征描述</th>
</tr>
</thead>
<tbody><tr>
<td>Claude 系列</td>
<td><strong>倾向弃权</strong></td>
<td>当不确定时，倾向于表示”无法回答”或”信息不足”</td>
</tr>
<tr>
<td>GPT 系列</td>
<td><strong>倾向幻觉</strong></td>
<td>当不确定时，倾向于生成看似合理但错误的答案</td>
</tr>
<tr>
<td>Gemini 系列</td>
<td><strong>混合模式</strong></td>
<td>介于两者之间，表现出不一致的行为</td>
</tr>
</tbody></table>
<p>这一发现对实际应用有重要启示：</p>
<ul>
<li>在<strong>高风险场景</strong>（如医疗、法律），Claude 的弃权倾向可能更安全</li>
<li>在<strong>用户体验优先</strong>的场景，GPT 的”总是给出答案”可能更受欢迎</li>
<li>但无论哪种模式，都反映了模型在长上下文下的能力边界</li>
</ul>
<hr>
<h2 id="第二部分：Context-Engineering-体系框架"><a href="#第二部分：Context-Engineering-体系框架" class="headerlink" title="第二部分：Context Engineering 体系框架"></a>第二部分：Context Engineering 体系框架</h2><p>在深入 StreamBench 之前，我们需要理解 <strong>Context Engineering</strong> 的整体框架，明确 StreamBench 解决的是其中哪个模块的问题。</p>
<h3 id="2-1-Context-Engineering-的定义"><a href="#2-1-Context-Engineering-的定义" class="headerlink" title="2.1 Context Engineering 的定义"></a>2.1 Context Engineering 的定义</h3><p>Context Engineering（上下文工程）是指<strong>系统性地设计、构建和优化 LLM 输入上下文的方法论和技术体系</strong>。</p>
<p>这个概念在 AI Agent 开发实践中逐渐浮现，Anthropic 在其 Agent 系统构建指南中首次系统性地提出了这一框架。它代表了从”单次提示优化”到”动态上下文管理”的范式升级。</p>
<h3 id="2-2-Context-Engineering-vs-Prompt-Engineering"><a href="#2-2-Context-Engineering-vs-Prompt-Engineering" class="headerlink" title="2.2 Context Engineering vs Prompt Engineering"></a>2.2 Context Engineering vs Prompt Engineering</h3><p>很多人容易混淆这两个概念，但它们在目标、范围和技术复杂度上存在本质区别：</p>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/prompt%20engineering%20vs%20context%20engineering.png" alt="Prompt Engineering vs Context Engineering"></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Prompt Engineering</th>
<th>Context Engineering</th>
</tr>
</thead>
<tbody><tr>
<td><strong>关注点</strong></td>
<td>单次提示的措辞和结构</td>
<td>跨多次交互的上下文构建与管理</td>
</tr>
<tr>
<td><strong>时间跨度</strong></td>
<td>静态、一次性</td>
<td>动态、持续演化</td>
</tr>
<tr>
<td><strong>信息来源</strong></td>
<td>人工编写的指令</td>
<td>外部检索、历史记忆、工具调用等多源融合</td>
</tr>
<tr>
<td><strong>核心问题</strong></td>
<td>“如何问得更好？”</td>
<td>“如何提供正确的信息？”</td>
</tr>
<tr>
<td><strong>典型技术</strong></td>
<td>思维链（CoT）、少样本示例、角色设定</td>
<td>RAG、Memory 管理、动态压缩、上下文路由</td>
</tr>
<tr>
<td><strong>失败模式</strong></td>
<td>模型误解意图</td>
<td>关键信息缺失或被淹没（Context Rot）</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>简单的问答、文本生成</td>
<td>复杂的 Agent 系统、多轮对话、知识密集型任务</td>
</tr>
</tbody></table>
<p><strong>一个形象的类比</strong>：</p>
<ul>
<li><strong>Prompt Engineering</strong> 像是写一封精心措辞的信</li>
<li><strong>Context Engineering</strong> 像是管理一个完整的图书馆系统——需要决定收藏哪些书（记忆管理）、如何分类（结构化）、如何快速找到需要的书（检索）、书架空间有限时如何取舍（压缩）</li>
</ul>
<p>Andrej Karpathy 在评价这一转变时提出：</p>
<blockquote>
<p>I think the space of prompt engineering will be replaced by the even bigger space of context engineering.</p>
</blockquote>
<p>这意味着，对于现代 AI Agent 开发者来说，<strong>仅仅会写好 Prompt 是远远不够的</strong>，更需要理解如何设计和优化整个上下文的生命周期。</p>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/prompt%20engineering%20%E5%92%8C%20context%20engineering.png" alt="Prompt Engineering 和 Context Engineering 的关系"></p>
<h3 id="2-3-Context-Engineering-的核心模块"><a href="#2-3-Context-Engineering-的核心模块" class="headerlink" title="2.3 Context Engineering 的核心模块"></a>2.3 Context Engineering 的核心模块</h3><p>一个完整的 Context Engineering 系统包含以下核心模块：</p>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/context%20engineering%20Framework.png" alt="Context Engineering Framework"></p>
<p><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/context%20engineering%20%E7%B1%BB%E5%9E%8B.png" alt="Context Engineering 类型"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">    subgraph CE[&quot;Context Engineering 体系&quot;]</span><br><span class="line">        direction TB</span><br><span class="line"></span><br><span class="line">        subgraph top[&quot;上层模块&quot;]</span><br><span class="line">            direction LR</span><br><span class="line">            A[&quot;Context Acquisition&lt;br/&gt;上下文获取&quot;]</span><br><span class="line">            B[&quot;Context Memory&lt;br/&gt;上下文记忆&quot;]</span><br><span class="line">            C[&quot;Context Compression&lt;br/&gt;上下文压缩&quot;]</span><br><span class="line">        end</span><br><span class="line"></span><br><span class="line">        subgraph bottom[&quot;下层模块&quot;]</span><br><span class="line">            direction LR</span><br><span class="line">            D[&quot;Context Retrieval&lt;br/&gt;上下文检索&quot;]</span><br><span class="line">            E[&quot;Context Routing&lt;br/&gt;上下文路由&quot;]</span><br><span class="line">            F[&quot;Context Optimization&lt;br/&gt;上下文优化&quot;]</span><br><span class="line">        end</span><br><span class="line"></span><br><span class="line">        A --&gt; D</span><br><span class="line">        B --&gt; E</span><br><span class="line">        C --&gt; F</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    style B fill:#e74c3c,stroke:#c0392b,color:#fff</span><br><span class="line">    style CE fill:#f8f9fa,stroke:#343a40</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>注</strong>：红色高亮的 <strong>Context Memory</strong> 模块正是 StreamBench 论文所聚焦的核心问题。</p>
</blockquote>
<h4 id="模块一：Context-Acquisition（上下文获取）"><a href="#模块一：Context-Acquisition（上下文获取）" class="headerlink" title="模块一：Context Acquisition（上下文获取）"></a>模块一：Context Acquisition（上下文获取）</h4><p><strong>问题</strong>：如何从外部世界获取相关信息？</p>
<p><strong>技术方案</strong>：</p>
<ul>
<li>RAG（检索增强生成）</li>
<li>Web Search</li>
<li>API 调用</li>
<li>工具使用</li>
</ul>
<h4 id="模块二：Context-Memory（上下文记忆）-lt-StreamBench-聚焦的模块"><a href="#模块二：Context-Memory（上下文记忆）-lt-StreamBench-聚焦的模块" class="headerlink" title="模块二：Context Memory（上下文记忆） &lt;- StreamBench 聚焦的模块"></a>模块二：Context Memory（上下文记忆） &lt;- StreamBench 聚焦的模块</h4><p><strong>问题</strong>：如何存储、组织和更新历史交互信息？</p>
<p><strong>核心挑战</strong>：</p>
<ul>
<li>记忆什么？（选择性存储）</li>
<li>如何组织？（结构化表示）</li>
<li>何时更新？（增量学习）</li>
<li>如何检索？（相关性匹配）</li>
</ul>
<p><strong>StreamBench 的贡献正是针对这一模块，特别是解决了”记忆什么”和”如何从记忆中学习”的问题。</strong></p>
<h4 id="模块三：Context-Compression（上下文压缩）"><a href="#模块三：Context-Compression（上下文压缩）" class="headerlink" title="模块三：Context Compression（上下文压缩）"></a>模块三：Context Compression（上下文压缩）</h4><p><strong>问题</strong>：如何在有限的上下文窗口内容纳更多有效信息？</p>
<p><strong>技术方案</strong>：</p>
<ul>
<li>文本摘要</li>
<li>信息蒸馏</li>
<li>Token 压缩</li>
<li>结构化表示</li>
</ul>
<h4 id="模块四：Context-Retrieval（上下文检索）"><a href="#模块四：Context-Retrieval（上下文检索）" class="headerlink" title="模块四：Context Retrieval（上下文检索）"></a>模块四：Context Retrieval（上下文检索）</h4><p><strong>问题</strong>：如何从存储的信息中高效检索相关内容？</p>
<p><strong>技术方案</strong>：</p>
<ul>
<li>向量相似度搜索</li>
<li>关键词匹配</li>
<li>混合检索</li>
<li>重排序（Reranking）</li>
</ul>
<h4 id="模块五：Context-Routing（上下文路由）"><a href="#模块五：Context-Routing（上下文路由）" class="headerlink" title="模块五：Context Routing（上下文路由）"></a>模块五：Context Routing（上下文路由）</h4><p><strong>问题</strong>：如何决定将上下文发送给哪个模型或处理流程？</p>
<p><strong>技术方案</strong>：</p>
<ul>
<li>模型路由</li>
<li>任务分发</li>
<li>多 Agent 协调</li>
</ul>
<h4 id="模块六：Context-Optimization（上下文优化）"><a href="#模块六：Context-Optimization（上下文优化）" class="headerlink" title="模块六：Context Optimization（上下文优化）"></a>模块六：Context Optimization（上下文优化）</h4><p><strong>问题</strong>：如何持续优化上下文的质量和效果？</p>
<p><strong>技术方案</strong>：</p>
<ul>
<li>反馈学习</li>
<li>A&#x2F;B 测试</li>
<li>效果评估</li>
</ul>
<h3 id="2-4-StreamBench-在体系中的定位"><a href="#2-4-StreamBench-在体系中的定位" class="headerlink" title="2.4 StreamBench 在体系中的定位"></a>2.4 StreamBench 在体系中的定位</h3><p>明确了 Context Engineering 的体系框架后，我们可以精确定位 StreamBench 的贡献：</p>
<p><strong>StreamBench 主要解决 Context Memory 模块中的以下问题</strong>：</p>
<ol>
<li><strong>选择性存储</strong>：应该将哪些交互历史存入记忆？</li>
<li><strong>记忆质量控制</strong>：如何确保存储的信息是高质量的？</li>
<li><strong>跨时间学习</strong>：如何让系统从历史经验中持续改进？</li>
<li><strong>多源记忆整合</strong>：如何整合来自不同 Agent 的经验？</li>
</ol>
<hr>
<h2 id="第三部分：StreamBench-系统性介绍"><a href="#第三部分：StreamBench-系统性介绍" class="headerlink" title="第三部分：StreamBench 系统性介绍"></a>第三部分：StreamBench 系统性介绍</h2><h3 id="3-1-研究动机与问题定义"><a href="#3-1-研究动机与问题定义" class="headerlink" title="3.1 研究动机与问题定义"></a>3.1 研究动机与问题定义</h3><h4 id="3-1-1-现有评估基准的局限"><a href="#3-1-1-现有评估基准的局限" class="headerlink" title="3.1.1 现有评估基准的局限"></a>3.1.1 现有评估基准的局限</h4><p>现有的 LLM 评估基准（如 MMLU、GSM8K、BIG-Bench-Hard）主要评估模型的<strong>固有能力（Innate Capabilities）</strong>——即模型开箱即用的表现。</p>
<p>但这种评估方式忽略了一个关键问题：<strong>在实际部署中，我们更希望系统能够随着时间推移不断改进。</strong></p>
<h4 id="3-1-2-StreamBench-的问题定义"><a href="#3-1-2-StreamBench-的问题定义" class="headerlink" title="3.1.2 StreamBench 的问题定义"></a>3.1.2 StreamBench 的问题定义</h4><p>StreamBench 提出了一个新的评估范式：<strong>在线流式学习场景下的 Agent 能力评估</strong>。</p>
<p>形式化定义：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：一个按时间顺序排列的实例序列 &#123;x₁, x₂, ..., xₜ, ...&#125;</span><br><span class="line">输出：对应的预测序列 &#123;ŷ₁, ŷ₂, ..., ŷₜ, ...&#125;</span><br><span class="line">反馈：二元信号 fbₜ ∈ &#123;0, 1&#125;，表示 ŷₜ 是否正确</span><br><span class="line">目标：最大化整个序列的累积准确率</span><br></pre></td></tr></table></figure>

<p>关键特征：</p>
<ul>
<li><strong>在线性</strong>：模型在每个时间步只能看到当前及之前的信息</li>
<li><strong>反馈驱动</strong>：模型可以利用反馈信号来改进后续预测</li>
<li><strong>累积评估</strong>：评估的是整个序列的总体表现，而非单点性能</li>
</ul>
<h3 id="3-2-Agent-架构设计"><a href="#3-2-Agent-架构设计" class="headerlink" title="3.2 Agent 架构设计"></a>3.2 Agent 架构设计</h3><p>StreamBench 采用了一个模块化的 Agent 架构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">    subgraph Agent[&quot;Agent 架构&quot;]</span><br><span class="line">        direction TB</span><br><span class="line"></span><br><span class="line">        Input[&quot;输入 xₜ&quot;]</span><br><span class="line"></span><br><span class="line">        subgraph MemorySystem[&quot;记忆系统&quot;]</span><br><span class="line">            direction LR</span><br><span class="line">            Retriever[&quot;检索器 r(·)&quot;]</span><br><span class="line">            Memory[&quot;外部记忆 M&quot;]</span><br><span class="line">            Retriever &lt;--&gt; Memory</span><br><span class="line">        end</span><br><span class="line"></span><br><span class="line">        Prompt[&quot;提示模板 p(·)&quot;]</span><br><span class="line">        LLM[&quot;LLM f(·|θ)&quot;]</span><br><span class="line">        Output[&quot;输出 ŷₜ&quot;]</span><br><span class="line"></span><br><span class="line">        Input --&gt; Retriever</span><br><span class="line">        Retriever --&gt; Prompt</span><br><span class="line">        Prompt --&gt; LLM</span><br><span class="line">        LLM --&gt; Output</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    style Memory fill:#3498db,stroke:#2980b9,color:#fff</span><br><span class="line">    style LLM fill:#2ecc71,stroke:#27ae60,color:#fff</span><br></pre></td></tr></table></figure>

<p><strong>核心组件</strong>：</p>
<ul>
<li>**LLM f(·|θ)**：基础语言模型</li>
<li><strong>外部记忆 M</strong>：存储历史交互信息</li>
<li>**检索器 r(·)**：从记忆中检索相关信息</li>
<li>**提示模板 p(·)**：将输入和检索结果组织成 LLM 输入</li>
</ul>
<p><strong>更新机制</strong>：<br>在每个时间步 t，Agent 可以更新以下组件：</p>
<ul>
<li>提示模板 p(·)</li>
<li>检索器 r(·)</li>
<li>外部记忆 M</li>
<li>（理论上也可以更新模型参数 θ，但成本较高）</li>
</ul>
<h3 id="3-3-基线方法对比"><a href="#3-3-基线方法对比" class="headerlink" title="3.3 基线方法对比"></a>3.3 基线方法对比</h3><p>StreamBench 设计了两类基线方法进行对比：</p>
<h4 id="3-3-1-非流式方法（Non-streaming-Methods）"><a href="#3-3-1-非流式方法（Non-streaming-Methods）" class="headerlink" title="3.3.1 非流式方法（Non-streaming Methods）"></a>3.3.1 非流式方法（Non-streaming Methods）</h4><p>这些方法关注单实例性能优化，不利用历史信息：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>Zero-shot</td>
<td>直接推理，无示例</td>
<td>最简单的基线</td>
</tr>
<tr>
<td>Few-shot</td>
<td>提供固定的示例</td>
<td>示例来自训练集</td>
</tr>
<tr>
<td>Chain-of-Thought</td>
<td>引导逐步推理</td>
<td>提升复杂任务性能</td>
</tr>
<tr>
<td>Self-Refine</td>
<td>自我反思和修正</td>
<td>单实例多轮优化</td>
</tr>
</tbody></table>
<h4 id="3-3-2-流式方法（Streaming-Methods）"><a href="#3-3-2-流式方法（Streaming-Methods）" class="headerlink" title="3.3.2 流式方法（Streaming Methods）"></a>3.3.2 流式方法（Streaming Methods）</h4><p>这些方法利用历史交互信息来改进未来预测：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>记忆内容</th>
<th>检索策略</th>
<th>核心思想</th>
</tr>
</thead>
<tbody><tr>
<td>GrowPrompt</td>
<td>(xₜ, ŷₜ, fbₜ)</td>
<td>滑动窗口</td>
<td>保留最近 k 个交互</td>
</tr>
<tr>
<td>MemPrompt</td>
<td>(xₜ, ŷₜ, fbₜ)</td>
<td>相似度检索</td>
<td>检索最相关的 k 个交互</td>
</tr>
<tr>
<td><strong>Self-StreamICL</strong></td>
<td>仅 (xₜ, ŷₜ) 当 fbₜ&#x3D;1</td>
<td>相似度检索</td>
<td><strong>只存储正确输出</strong></td>
</tr>
<tr>
<td><strong>MAM-StreamICL</strong></td>
<td>多 Agent 共享记忆</td>
<td>相似度检索</td>
<td><strong>跨 Agent 经验共享</strong></td>
</tr>
</tbody></table>
<h3 id="3-4-核心方法详解"><a href="#3-4-核心方法详解" class="headerlink" title="3.4 核心方法详解"></a>3.4 核心方法详解</h3><h4 id="3-4-1-Self-StreamICL：选择性记忆的威力"><a href="#3-4-1-Self-StreamICL：选择性记忆的威力" class="headerlink" title="3.4.1 Self-StreamICL：选择性记忆的威力"></a>3.4.1 Self-StreamICL：选择性记忆的威力</h4><p><strong>核心洞察</strong>：错误的示例会干扰模型学习，即使明确标注为”错误”也无法完全消除干扰。</p>
<p><strong>算法流程</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_stream_icl</span>(<span class="params">agent, memory, retriever, data_stream</span>):</span><br><span class="line">    <span class="keyword">for</span> t, x_t <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_stream):</span><br><span class="line">        <span class="comment"># 1. 从记忆中检索相关示例</span></span><br><span class="line">        retrieved = retriever.retrieve(memory, x_t, k=K)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 构建提示并生成预测</span></span><br><span class="line">        prompt = build_prompt(x_t, retrieved)</span><br><span class="line">        y_hat_t = agent.generate(prompt)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 获取反馈</span></span><br><span class="line">        fb_t = environment.get_feedback(x_t, y_hat_t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 选择性存储：仅当预测正确时才存入记忆</span></span><br><span class="line">        <span class="keyword">if</span> fb_t == <span class="number">1</span>:  <span class="comment"># 正确</span></span><br><span class="line">            memory.store(x_t, y_hat_t)</span><br><span class="line">        <span class="comment"># 错误的预测不存储！</span></span><br></pre></td></tr></table></figure>

<p><strong>为什么有效？</strong></p>
<ol>
<li><p><strong>避免负面示例干扰</strong>：</p>
<ul>
<li>研究表明，即使通过语言告知模型”这个答案是错的”，模型仍会受到错误答案的干扰</li>
<li>Self-StreamICL 直接避免将错误信息存入记忆</li>
</ul>
</li>
<li><p><strong>自我强化学习</strong>：</p>
<ul>
<li>正确的输出被用作未来推理的示例</li>
<li>形成一个正向反馈循环</li>
</ul>
</li>
<li><p><strong>隐式课程学习</strong>：</p>
<ul>
<li>随着记忆积累，模型获得越来越多的高质量示例</li>
<li>类似于从简单到困难的课程学习</li>
</ul>
</li>
</ol>
<p><strong>消融实验验证</strong>：</p>
<table>
<thead>
<tr>
<th>记忆策略</th>
<th>相对于 Zero-shot 的性能变化</th>
</tr>
</thead>
<tbody><tr>
<td>仅使用正确输出</td>
<td>↑ 显著提升</td>
</tr>
<tr>
<td>使用所有输出（带标注）</td>
<td>↑ 小幅提升或持平</td>
</tr>
<tr>
<td>仅使用错误输出</td>
<td>↓ 显著下降（甚至不如 Zero-shot）</td>
</tr>
</tbody></table>
<h4 id="3-4-2-MAM-StreamICL：多智能体记忆共享"><a href="#3-4-2-MAM-StreamICL：多智能体记忆共享" class="headerlink" title="3.4.2 MAM-StreamICL：多智能体记忆共享"></a>3.4.2 MAM-StreamICL：多智能体记忆共享</h4><p><strong>核心洞察</strong>：不同的 LLM 具有不同的优势领域，共享记忆可以让每个 Agent 受益于其他 Agent 的专长。</p>
<p><strong>算法流程</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mam_stream_icl</span>(<span class="params">agents, shared_memory, retriever, data_stream</span>):</span><br><span class="line">    K = <span class="built_in">len</span>(agents)  <span class="comment"># Agent 数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t, x_t <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_stream):</span><br><span class="line">        <span class="comment"># 1. 轮询选择当前时间步的 Agent</span></span><br><span class="line">        current_agent = agents[t % K]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 从共享记忆中检索</span></span><br><span class="line">        retrieved = retriever.retrieve(shared_memory, x_t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 当前 Agent 生成预测</span></span><br><span class="line">        prompt = build_prompt(x_t, retrieved)</span><br><span class="line">        y_hat_t = current_agent.generate(prompt)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 获取反馈并选择性存储到共享记忆</span></span><br><span class="line">        fb_t = environment.get_feedback(x_t, y_hat_t)</span><br><span class="line">        <span class="keyword">if</span> fb_t == <span class="number">1</span>:</span><br><span class="line">            shared_memory.store(x_t, y_hat_t)</span><br></pre></td></tr></table></figure>

<p><strong>为什么有效？</strong></p>
<ol>
<li><p><strong>能力互补</strong>：<br>以医疗诊断（DDXPlus）为例：</p>
<ul>
<li>GPT-3.5-turbo 擅长：急性鼻窦炎、过敏性鼻窦炎</li>
<li>Gemini-1.0-pro 擅长：上呼吸道感染</li>
<li>Claude-3-haiku 擅长：慢性鼻窦炎</li>
</ul>
<p>通过共享记忆，每个 Agent 都能从其他 Agent 的成功经验中学习。</p>
</li>
<li><p><strong>成本效益</strong>：</p>
<ul>
<li>传统多 Agent 方法（如 Multi-Agent Debate）成本随 Agent 数量线性增长</li>
<li>MAM-StreamICL 的成本约等于<strong>单个 Agent 的平均成本</strong></li>
<li>因为每个时间步只有一个 Agent 在工作</li>
</ul>
</li>
<li><p><strong>多样性红利</strong>：</p>
<ul>
<li>不同模型的失败模式不同（如 Claude 倾向弃权，GPT 倾向幻觉）</li>
<li>共享记忆平滑了这些差异，提高整体鲁棒性</li>
</ul>
</li>
</ol>
<h3 id="3-5-数据集与评估"><a href="#3-5-数据集与评估" class="headerlink" title="3.5 数据集与评估"></a>3.5 数据集与评估</h3><h4 id="3-5-1-任务覆盖"><a href="#3-5-1-任务覆盖" class="headerlink" title="3.5.1 任务覆盖"></a>3.5.1 任务覆盖</h4><p>StreamBench 涵盖五类具有实际应用价值的任务：</p>
<table>
<thead>
<tr>
<th>任务</th>
<th>数据集</th>
<th>规模</th>
<th>评估指标</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>Text-to-SQL</td>
<td>Spider, CoSQL, BIRD</td>
<td>2147&#x2F;1007&#x2F;1534</td>
<td>执行准确率</td>
<td>数据分析助手</td>
</tr>
<tr>
<td>Python 编程</td>
<td>DS-1000</td>
<td>1000</td>
<td>Pass@1</td>
<td>代码生成</td>
</tr>
<tr>
<td>工具使用</td>
<td>ToolBench</td>
<td>750</td>
<td>准确率</td>
<td>API 调用</td>
</tr>
<tr>
<td>医疗诊断</td>
<td>DDXPlus</td>
<td>1764</td>
<td>准确率</td>
<td>医疗辅助</td>
</tr>
<tr>
<td>问答</td>
<td>HotpotQA</td>
<td>1500</td>
<td>精确匹配</td>
<td>知识问答</td>
</tr>
</tbody></table>
<h4 id="3-5-2-反馈信号设计"><a href="#3-5-2-反馈信号设计" class="headerlink" title="3.5.2 反馈信号设计"></a>3.5.2 反馈信号设计</h4><p>StreamBench 采用<strong>二元反馈</strong>（正确&#x2F;错误），这是基于以下考量：</p>
<ol>
<li><p><strong>实用性</strong>：在实际应用中，获取精确的 ground truth 往往成本很高，但简单的正确&#x2F;错误反馈（如点赞&#x2F;点踩）很容易收集</p>
</li>
<li><p><strong>通用性</strong>：二元反馈是最基础的反馈形式，可以统一不同任务的评估</p>
</li>
<li><p><strong>可扩展性</strong>：未来研究可以探索更丰富的反馈形式（如自然语言反馈、置信度分数等）</p>
</li>
</ol>
<h3 id="3-6-实验结果与分析"><a href="#3-6-实验结果与分析" class="headerlink" title="3.6 实验结果与分析"></a>3.6 实验结果与分析</h3><h4 id="3-6-1-主要结果"><a href="#3-6-1-主要结果" class="headerlink" title="3.6.1 主要结果"></a>3.6.1 主要结果</h4><p>在三个 LLM（GPT-3.5-turbo、Gemini-1.0-pro、Claude-3-haiku）上的平均结果：</p>
<table>
<thead>
<tr>
<th>方法类型</th>
<th>方法</th>
<th>Spider</th>
<th>DDXPlus</th>
<th>HotpotQA</th>
</tr>
</thead>
<tbody><tr>
<td>非流式</td>
<td>Zero-shot</td>
<td>67.89</td>
<td>52.85</td>
<td>48.49</td>
</tr>
<tr>
<td>非流式</td>
<td>Few-shot</td>
<td>68.55</td>
<td>60.98</td>
<td>53.11</td>
</tr>
<tr>
<td>非流式</td>
<td>Self-Refine</td>
<td>67.75</td>
<td>52.89</td>
<td>43.53</td>
</tr>
<tr>
<td>流式</td>
<td>GrowPrompt</td>
<td>69.90</td>
<td>55.10</td>
<td>51.38</td>
</tr>
<tr>
<td>流式</td>
<td>MemPrompt</td>
<td>70.78</td>
<td>54.02</td>
<td>52.62</td>
</tr>
<tr>
<td>流式</td>
<td>Self-StreamICL</td>
<td><strong>74.63</strong></td>
<td>70.56</td>
<td>54.80</td>
</tr>
<tr>
<td>流式</td>
<td>MAM-StreamICL</td>
<td><strong>75.69</strong></td>
<td><strong>83.50</strong></td>
<td><strong>55.20</strong></td>
</tr>
</tbody></table>
<p><strong>关键观察</strong>：</p>
<ol>
<li><strong>流式方法全面优于非流式方法</strong></li>
<li><strong>Self-StreamICL 显著优于其他流式基线</strong>（验证了选择性记忆的重要性）</li>
<li><strong>MAM-StreamICL 在所有任务上取得最佳性能</strong>（多 Agent 协作的价值）</li>
<li><strong>DDXPlus 任务上的提升最为显著</strong>（从 52.85 到 83.50，提升 58%）</li>
</ol>
<h4 id="3-6-2-强模型同样受益"><a href="#3-6-2-强模型同样受益" class="headerlink" title="3.6.2 强模型同样受益"></a>3.6.2 强模型同样受益</h4><p>即使是更强的模型（GPT-4o、Gemini-1.5-flash），也能从流式学习中获益：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>方法</th>
<th>Spider</th>
<th>DDXPlus</th>
</tr>
</thead>
<tbody><tr>
<td>GPT-4o</td>
<td>Zero-shot</td>
<td>83.2</td>
<td>74.5</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>Self-StreamICL</td>
<td><strong>88.1</strong></td>
<td><strong>89.3</strong></td>
</tr>
<tr>
<td>Gemini-1.5-flash</td>
<td>Zero-shot</td>
<td>81.7</td>
<td>71.2</td>
</tr>
<tr>
<td>Gemini-1.5-flash</td>
<td>Self-StreamICL</td>
<td><strong>86.5</strong></td>
<td><strong>87.8</strong></td>
</tr>
</tbody></table>
<p>这表明<strong>流式学习策略是普适的</strong>，不依赖于特定的基础模型能力。</p>
<h4 id="3-6-3-成本分析"><a href="#3-6-3-成本分析" class="headerlink" title="3.6.3 成本分析"></a>3.6.3 成本分析</h4><p>StreamBench 还分析了不同方法的 token 消耗：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>输入 Token (M)</th>
<th>输出 Token (M)</th>
<th>相对成本</th>
</tr>
</thead>
<tbody><tr>
<td>Zero-shot</td>
<td>0.87</td>
<td>0.11</td>
<td>1.0x</td>
</tr>
<tr>
<td>Self-Refine</td>
<td>1.94</td>
<td>0.13</td>
<td>2.1x</td>
</tr>
<tr>
<td>Self-StreamICL</td>
<td>2.84</td>
<td>0.10</td>
<td>3.0x</td>
</tr>
<tr>
<td>MAM-StreamICL</td>
<td>2.84</td>
<td>0.10</td>
<td><strong>3.0x</strong></td>
</tr>
</tbody></table>
<p><strong>重要发现</strong>：MAM-StreamICL 虽然使用多个 Agent，但成本与 Self-StreamICL 相当（因为每个时间步只有一个 Agent 工作），却获得了显著的性能提升。</p>
<hr>
<h2 id="第四部分：Context-Rot-与-StreamBench-的关联"><a href="#第四部分：Context-Rot-与-StreamBench-的关联" class="headerlink" title="第四部分：Context Rot 与 StreamBench 的关联"></a>第四部分：Context Rot 与 StreamBench 的关联</h2><h3 id="4-1-问题与解决方案的对应"><a href="#4-1-问题与解决方案的对应" class="headerlink" title="4.1 问题与解决方案的对应"></a>4.1 问题与解决方案的对应</h3><p>Context Rot 和 StreamBench 从不同角度切入了 LLM 上下文处理的挑战：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Context Rot</th>
<th>StreamBench</th>
</tr>
</thead>
<tbody><tr>
<td>性质</td>
<td>问题诊断</td>
<td>解决方案</td>
</tr>
<tr>
<td>视角</td>
<td>静态分析</td>
<td>动态优化</td>
</tr>
<tr>
<td>关注点</td>
<td>上下文长度对性能的影响</td>
<td>如何选择性管理上下文</td>
</tr>
<tr>
<td>核心洞察</td>
<td>上下文质量比数量更重要</td>
<td>只记忆正确信息</td>
</tr>
</tbody></table>
<h3 id="4-2-StreamBench-如何应对-Context-Rot"><a href="#4-2-StreamBench-如何应对-Context-Rot" class="headerlink" title="4.2 StreamBench 如何应对 Context Rot"></a>4.2 StreamBench 如何应对 Context Rot</h3><p>StreamBench 的设计理念直接回应了 Context Rot 揭示的问题：</p>
<p><strong>Context Rot 问题</strong>：上下文越长，性能越差<br><strong>StreamBench 策略</strong>：不追求上下文长度，而是追求<strong>上下文质量</strong></p>
<p><strong>Context Rot 问题</strong>：干扰项会显著影响性能<br><strong>StreamBench 策略</strong>：<strong>只存储正确输出</strong>，从源头消除干扰</p>
<p><strong>Context Rot 问题</strong>：注意力稀释导致关键信息被忽视<br><strong>StreamBench 策略</strong>：<strong>相似度检索</strong>确保最相关的信息进入上下文</p>
<h3 id="4-3-对-Context-Engineering-的启示"><a href="#4-3-对-Context-Engineering-的启示" class="headerlink" title="4.3 对 Context Engineering 的启示"></a>4.3 对 Context Engineering 的启示</h3><p>两项研究共同指向了以下 Context Engineering 原则：</p>
<ol>
<li><p><strong>质量优先于数量</strong></p>
<ul>
<li>不要盲目追求长上下文</li>
<li>精心筛选的少量高质量信息优于大量混杂信息</li>
</ul>
</li>
<li><p><strong>选择性记忆</strong></p>
<ul>
<li>实施严格的记忆准入机制</li>
<li>错误信息的危害可能大于收益</li>
</ul>
</li>
<li><p><strong>动态适应</strong></p>
<ul>
<li>上下文策略应该随着交互不断优化</li>
<li>利用反馈信号进行持续改进</li>
</ul>
</li>
<li><p><strong>多源整合</strong></p>
<ul>
<li>不同来源的信息可以互补</li>
<li>多 Agent 架构可以提供多样性红利</li>
</ul>
</li>
</ol>
<hr>
<h2 id="第五部分：实践建议"><a href="#第五部分：实践建议" class="headerlink" title="第五部分：实践建议"></a>第五部分：实践建议</h2><h3 id="5-1-针对-Context-Rot-的缓解策略"><a href="#5-1-针对-Context-Rot-的缓解策略" class="headerlink" title="5.1 针对 Context Rot 的缓解策略"></a>5.1 针对 Context Rot 的缓解策略</h3><ol>
<li><p><strong>控制上下文长度</strong></p>
<ul>
<li>不要盲目使用最大上下文窗口</li>
<li>建立上下文长度与任务性能的关系曲线</li>
<li>找到性能-成本的最优平衡点</li>
</ul>
</li>
<li><p><strong>优化信息布局</strong></p>
<ul>
<li>将关键信息放在上下文的开头或结尾</li>
<li>避免关键信息被淹没在中间位置</li>
</ul>
</li>
<li><p><strong>减少干扰信息</strong></p>
<ul>
<li>实施严格的相关性过滤</li>
<li>避免引入与查询高度相似但无关的内容</li>
</ul>
</li>
</ol>
<h3 id="5-2-基于-StreamBench-的系统设计"><a href="#5-2-基于-StreamBench-的系统设计" class="headerlink" title="5.2 基于 StreamBench 的系统设计"></a>5.2 基于 StreamBench 的系统设计</h3><ol>
<li><p><strong>实施选择性记忆</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">should_store</span>(<span class="params">interaction</span>):</span><br><span class="line">    <span class="comment"># 方式一：基于用户反馈</span></span><br><span class="line">    <span class="keyword">if</span> interaction.user_feedback == <span class="string">&quot;positive&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方式二：基于自动验证</span></span><br><span class="line">    <span class="keyword">if</span> interaction.task_type == <span class="string">&quot;code&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> run_tests(interaction.output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方式三：基于置信度</span></span><br><span class="line">    <span class="keyword">if</span> interaction.model_confidence &gt; <span class="number">0.9</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>构建多 Agent 记忆共享系统</strong></p>
<ul>
<li>部署多个专业化 Agent</li>
<li>设计共享记忆的存储和检索机制</li>
<li>实施轮询或动态调度策略</li>
</ul>
</li>
<li><p><strong>建立持续评估机制</strong></p>
<ul>
<li>跟踪系统在不同上下文长度下的表现</li>
<li>监控记忆质量和检索效果</li>
<li>定期审计和清理低质量记忆</li>
</ul>
</li>
</ol>
<h3 id="5-3-评估指标建议"><a href="#5-3-评估指标建议" class="headerlink" title="5.3 评估指标建议"></a>5.3 评估指标建议</h3><p>除了准确率，还应关注：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>上下文效率</td>
<td>每 token 带来的性能增益</td>
<td>成本优化</td>
</tr>
<tr>
<td>记忆利用率</td>
<td>检索的记忆被实际使用的比例</td>
<td>记忆质量</td>
</tr>
<tr>
<td>改进曲线</td>
<td>性能随交互次数的变化</td>
<td>学习能力</td>
</tr>
<tr>
<td>鲁棒性</td>
<td>在不同输入顺序下的性能稳定性</td>
<td>可靠性</td>
</tr>
</tbody></table>
<hr>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Context Rot 揭示了 LLM 在长上下文处理中的根本性局限——这不仅仅是”上下文太长”的问题，而是 Transformer 注意力机制的固有约束所导致的结构性挑战。</p>
<p>StreamBench 则提供了一种务实的应对策略：与其试图让模型处理越来越长的上下文，不如<strong>精心管理上下文的质量</strong>。通过选择性记忆和流式学习，即使在有限的上下文窗口下，系统也能持续改进并展现出色的性能。</p>
<p>这两项研究共同指向了 Context Engineering 的核心理念：</p>
<blockquote>
<p><strong>上下文工程的艺术，不在于”放多少”，而在于”放什么”和”怎么放”。</strong></p>
</blockquote>
<p>随着 AI Agent 应用的普及，有效的 Context Engineering 将成为区分优秀系统与普通系统的关键因素。理解 Context Rot 的机制，借鉴 StreamBench 的设计理念，将帮助我们构建更加可靠、高效的 LLM 应用。</p>
<hr>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol>
<li><a target="_blank" rel="noopener" href="https://research.trychroma.com/context-rot">Context Rot: How Increasing Input Tokens Impacts LLM Performance - Chroma Research</a></li>
<li><a target="_blank" rel="noopener" href="https://www.understandingai.org/p/context-rot-the-emerging-challenge">Context rot: the emerging challenge that could hold back LLM progress - Understanding AI</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.08747">StreamBench: Towards Benchmarking Continuous Improvement of Language Agents - arXiv</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/stream-bench/stream-bench">StreamBench GitHub Repository</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/appier-ai-research/StreamBench">StreamBench Dataset - Hugging Face</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.whispergenie.com">Scharfsinnig</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.whispergenie.com/post/62630.html">https://blog.whispergenie.com/post/62630.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.whispergenie.com" target="_blank">SCHARFSINNIGの博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI-Agent/">AI Agent</a><a class="post-meta__tags" href="/tags/Context-Engineering/">Context Engineering</a><a class="post-meta__tags" href="/tags/Context-Rot/">Context Rot</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87/">长上下文</a></div><div class="post_share"><div class="social-share" data-image="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/context%20engineering%E5%AF%BC%E8%88%AA%E5%A4%B4.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-card"><div class="reward-heading"><i class="fas fa-leaf"></i><span>随缘一杯茶</span></div><p class="reward-desc">若这些文字曾在某个时刻与你相和，如同一盏清茶带来一分安定与回甘；你若有心，且请我饮茶一杯，让创作在茶香里慢慢生长——随缘赞赏，心安即可。</p><ul class="reward-qr-grid"><li class="reward-pay-item"><a href="/images/qr-code/wechat-qrcode.png" target="_blank" aria-label="微信"><img src="/images/qr-code/wechat-qrcode.png" alt="微信" loading="lazy" width="200" height="200"/></a><span class="reward-pay-label">微信</span></li><li class="reward-pay-item"><a href="/images/qr-code/alipay-qrcode.png" target="_blank" aria-label="支付宝"><img src="/images/qr-code/alipay-qrcode.png" alt="支付宝" loading="lazy" width="200" height="200"/></a><span class="reward-pay-label">支付宝</span></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/20250107.html"><img class="prev-cover" src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%93%8D/ubuntu24.04%E4%B8%AD%E6%96%87%E8%BE%93%E5%85%A5%E6%B3%95%E7%BB%88%E6%9E%81%E6%8C%87%E5%8D%97/image.png" onerror="onerror=null;src='/images/ui/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Ubuntu 24.04 中文输入法终极指南</div></div></a></div><div class="next-post pull-right"><a href="/post/33350.html"><img class="next-cover" src="/images/blogs/%E5%8F%A4%E5%85%B8%E6%99%BA%E6%85%A7/%E8%8A%82%E6%B0%94%E4%B8%8E%E9%98%B4%E9%98%B3%E5%90%88%E5%8E%86/%E4%BA%8C%E5%8D%81%E5%9B%9B%E8%8A%82%E6%B0%94.png" onerror="onerror=null;src='/images/ui/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">节气与阴阳合历</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/post/62629.html" title="AI Agent：演变、架构与实际应用"><img class="cover" src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/AI-Agent-%E6%BC%94%E5%8F%98%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/image-01.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-10</div><div class="title">AI Agent：演变、架构与实际应用</div></div></a></div><div><a href="/post/64926.html" title="RAG系统性调研：范式、方法与工程化路径"><img class="cover" src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/RAG-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%B0%83%E7%A0%94/image-01.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-10</div><div class="title">RAG系统性调研：范式、方法与工程化路径</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatars/new_avator.png" onerror="this.onerror=null;this.src='/img/index/avator.png'" alt="avatar"/></div><div class="author-info__name">Scharfsinnig</div><div class="author-info__description">博学、慎思、明辨、笃行 | O ever youthful.O ever weeping.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">48</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/scharfsinnig"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/scharfsinnig" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:admin@whispergenie.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9AContext-Rot-%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">第一部分：Context Rot 深度剖析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF-Context-Rot%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 什么是 Context Rot？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Context-Rot-%E7%9A%84%E6%A0%B9%E6%9C%AC%E5%8E%9F%E5%9B%A0"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 Context Rot 的根本原因</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1-Transformer-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%9B%BA%E6%9C%89%E7%BA%A6%E6%9D%9F"><span class="toc-number">2.2.1.</span> <span class="toc-text">1.2.1 Transformer 注意力机制的固有约束</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">2.2.2.</span> <span class="toc-text">1.2.2 位置编码的局限性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2-vs-%E4%BF%A1%E6%81%AF%E6%95%B4%E5%90%88%E7%9A%84%E5%B7%AE%E5%BC%82"><span class="toc-number">2.2.3.</span> <span class="toc-text">1.2.3 信息检索 vs 信息整合的差异</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Chroma-%E7%9A%84%E7%B3%BB%E7%BB%9F%E6%80%A7%E5%AE%9E%E9%AA%8C"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 Chroma 的系统性实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%B8%80%EF%BC%9ANeedle-Question-%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6"><span class="toc-number">2.3.1.</span> <span class="toc-text">实验一：Needle-Question 语义相似度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%BA%8C%EF%BC%9A%E5%B9%B2%E6%89%B0%E9%A1%B9%E7%9A%84%E9%9D%9E%E5%9D%87%E5%8C%80%E5%BD%B1%E5%93%8D"><span class="toc-number">2.3.2.</span> <span class="toc-text">实验二：干扰项的非均匀影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9AHaystack-%E7%BB%93%E6%9E%84%E7%9A%84%E5%8F%8D%E7%9B%B4%E8%A7%89%E5%BD%B1%E5%93%8D"><span class="toc-number">2.3.3.</span> <span class="toc-text">实验三：Haystack 结构的反直觉影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%9B%9B%EF%BC%9A%E9%87%8D%E5%A4%8D%E5%8D%95%E8%AF%8D%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.3.4.</span> <span class="toc-text">实验四：重复单词任务</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E4%B8%8D%E5%90%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%B1%E8%B4%A5%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.4.</span> <span class="toc-text">1.4 不同模型的失败模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9AContext-Engineering-%E4%BD%93%E7%B3%BB%E6%A1%86%E6%9E%B6"><span class="toc-number">3.</span> <span class="toc-text">第二部分：Context Engineering 体系框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Context-Engineering-%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 Context Engineering 的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Context-Engineering-vs-Prompt-Engineering"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 Context Engineering vs Prompt Engineering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Context-Engineering-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 Context Engineering 的核心模块</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E4%B8%80%EF%BC%9AContext-Acquisition%EF%BC%88%E4%B8%8A%E4%B8%8B%E6%96%87%E8%8E%B7%E5%8F%96%EF%BC%89"><span class="toc-number">3.3.1.</span> <span class="toc-text">模块一：Context Acquisition（上下文获取）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E4%BA%8C%EF%BC%9AContext-Memory%EF%BC%88%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AE%B0%E5%BF%86%EF%BC%89-lt-StreamBench-%E8%81%9A%E7%84%A6%E7%9A%84%E6%A8%A1%E5%9D%97"><span class="toc-number">3.3.2.</span> <span class="toc-text">模块二：Context Memory（上下文记忆） &lt;- StreamBench 聚焦的模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E4%B8%89%EF%BC%9AContext-Compression%EF%BC%88%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8E%8B%E7%BC%A9%EF%BC%89"><span class="toc-number">3.3.3.</span> <span class="toc-text">模块三：Context Compression（上下文压缩）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E5%9B%9B%EF%BC%9AContext-Retrieval%EF%BC%88%E4%B8%8A%E4%B8%8B%E6%96%87%E6%A3%80%E7%B4%A2%EF%BC%89"><span class="toc-number">3.3.4.</span> <span class="toc-text">模块四：Context Retrieval（上下文检索）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E4%BA%94%EF%BC%9AContext-Routing%EF%BC%88%E4%B8%8A%E4%B8%8B%E6%96%87%E8%B7%AF%E7%94%B1%EF%BC%89"><span class="toc-number">3.3.5.</span> <span class="toc-text">模块五：Context Routing（上下文路由）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E5%85%AD%EF%BC%9AContext-Optimization%EF%BC%88%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BC%98%E5%8C%96%EF%BC%89"><span class="toc-number">3.3.6.</span> <span class="toc-text">模块六：Context Optimization（上下文优化）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-StreamBench-%E5%9C%A8%E4%BD%93%E7%B3%BB%E4%B8%AD%E7%9A%84%E5%AE%9A%E4%BD%8D"><span class="toc-number">3.4.</span> <span class="toc-text">2.4 StreamBench 在体系中的定位</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9AStreamBench-%E7%B3%BB%E7%BB%9F%E6%80%A7%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.</span> <span class="toc-text">第三部分：StreamBench 系统性介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%A0%94%E7%A9%B6%E5%8A%A8%E6%9C%BA%E4%B8%8E%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 研究动机与问题定义</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-%E7%8E%B0%E6%9C%89%E8%AF%84%E4%BC%B0%E5%9F%BA%E5%87%86%E7%9A%84%E5%B1%80%E9%99%90"><span class="toc-number">4.1.1.</span> <span class="toc-text">3.1.1 现有评估基准的局限</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-StreamBench-%E7%9A%84%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="toc-number">4.1.2.</span> <span class="toc-text">3.1.2 StreamBench 的问题定义</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 Agent 架构设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%9F%BA%E7%BA%BF%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 基线方法对比</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-%E9%9D%9E%E6%B5%81%E5%BC%8F%E6%96%B9%E6%B3%95%EF%BC%88Non-streaming-Methods%EF%BC%89"><span class="toc-number">4.3.1.</span> <span class="toc-text">3.3.1 非流式方法（Non-streaming Methods）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-%E6%B5%81%E5%BC%8F%E6%96%B9%E6%B3%95%EF%BC%88Streaming-Methods%EF%BC%89"><span class="toc-number">4.3.2.</span> <span class="toc-text">3.3.2 流式方法（Streaming Methods）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="toc-number">4.4.</span> <span class="toc-text">3.4 核心方法详解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1-Self-StreamICL%EF%BC%9A%E9%80%89%E6%8B%A9%E6%80%A7%E8%AE%B0%E5%BF%86%E7%9A%84%E5%A8%81%E5%8A%9B"><span class="toc-number">4.4.1.</span> <span class="toc-text">3.4.1 Self-StreamICL：选择性记忆的威力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-2-MAM-StreamICL%EF%BC%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E8%AE%B0%E5%BF%86%E5%85%B1%E4%BA%AB"><span class="toc-number">4.4.2.</span> <span class="toc-text">3.4.2 MAM-StreamICL：多智能体记忆共享</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-number">4.5.</span> <span class="toc-text">3.5 数据集与评估</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-1-%E4%BB%BB%E5%8A%A1%E8%A6%86%E7%9B%96"><span class="toc-number">4.5.1.</span> <span class="toc-text">3.5.1 任务覆盖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-2-%E5%8F%8D%E9%A6%88%E4%BF%A1%E5%8F%B7%E8%AE%BE%E8%AE%A1"><span class="toc-number">4.5.2.</span> <span class="toc-text">3.5.2 反馈信号设计</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E4%B8%8E%E5%88%86%E6%9E%90"><span class="toc-number">4.6.</span> <span class="toc-text">3.6 实验结果与分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-1-%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%9C"><span class="toc-number">4.6.1.</span> <span class="toc-text">3.6.1 主要结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-2-%E5%BC%BA%E6%A8%A1%E5%9E%8B%E5%90%8C%E6%A0%B7%E5%8F%97%E7%9B%8A"><span class="toc-number">4.6.2.</span> <span class="toc-text">3.6.2 强模型同样受益</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-3-%E6%88%90%E6%9C%AC%E5%88%86%E6%9E%90"><span class="toc-number">4.6.3.</span> <span class="toc-text">3.6.3 成本分析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9AContext-Rot-%E4%B8%8E-StreamBench-%E7%9A%84%E5%85%B3%E8%81%94"><span class="toc-number">5.</span> <span class="toc-text">第四部分：Context Rot 与 StreamBench 的关联</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E5%AF%B9%E5%BA%94"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 问题与解决方案的对应</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-StreamBench-%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9-Context-Rot"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 StreamBench 如何应对 Context Rot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%AF%B9-Context-Engineering-%E7%9A%84%E5%90%AF%E7%A4%BA"><span class="toc-number">5.3.</span> <span class="toc-text">4.3 对 Context Engineering 的启示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%EF%BC%9A%E5%AE%9E%E8%B7%B5%E5%BB%BA%E8%AE%AE"><span class="toc-number">6.</span> <span class="toc-text">第五部分：实践建议</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E9%92%88%E5%AF%B9-Context-Rot-%E7%9A%84%E7%BC%93%E8%A7%A3%E7%AD%96%E7%95%A5"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 针对 Context Rot 的缓解策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%9F%BA%E4%BA%8E-StreamBench-%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 基于 StreamBench 的系统设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E5%BB%BA%E8%AE%AE"><span class="toc-number">6.3.</span> <span class="toc-text">5.3 评估指标建议</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AF%AD"><span class="toc-number">7.</span> <span class="toc-text">结语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">8.</span> <span class="toc-text">参考链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><div class="content"><a class="title" href="/post/0.html" title="无题">无题</a><time datetime="2026-01-08T04:33:10.161Z" title="发表于 2026-01-08 12:33:10">2026-01-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/20250107.html" title="Ubuntu 24.04 中文输入法终极指南"><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%93%8D/ubuntu24.04%E4%B8%AD%E6%96%87%E8%BE%93%E5%85%A5%E6%B3%95%E7%BB%88%E6%9E%81%E6%8C%87%E5%8D%97/image.png" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="Ubuntu 24.04 中文输入法终极指南"/></a><div class="content"><a class="title" href="/post/20250107.html" title="Ubuntu 24.04 中文输入法终极指南">Ubuntu 24.04 中文输入法终极指南</a><time datetime="2026-01-07T07:27:00.000Z" title="发表于 2026-01-07 15:27:00">2026-01-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/62630.html" title="Context Engineering学习笔记：理解 Context Rot 与 StreamBench"><img src="/images/blogs/%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F/Context%20Engineering%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%90%86%E8%A7%A3%20Context%20Rot%20%E4%B8%8E%20StreamBench/context%20engineering%E5%AF%BC%E8%88%AA%E5%A4%B4.png" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="Context Engineering学习笔记：理解 Context Rot 与 StreamBench"/></a><div class="content"><a class="title" href="/post/62630.html" title="Context Engineering学习笔记：理解 Context Rot 与 StreamBench">Context Engineering学习笔记：理解 Context Rot 与 StreamBench</a><time datetime="2025-12-17T13:56:00.000Z" title="发表于 2025-12-17 21:56:00">2025-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/33350.html" title="节气与阴阳合历"><img src="/images/blogs/%E5%8F%A4%E5%85%B8%E6%99%BA%E6%85%A7/%E8%8A%82%E6%B0%94%E4%B8%8E%E9%98%B4%E9%98%B3%E5%90%88%E5%8E%86/%E4%BA%8C%E5%8D%81%E5%9B%9B%E8%8A%82%E6%B0%94.png" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="节气与阴阳合历"/></a><div class="content"><a class="title" href="/post/33350.html" title="节气与阴阳合历">节气与阴阳合历</a><time datetime="2025-11-27T15:00:00.000Z" title="发表于 2025-11-27 23:00:00">2025-11-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/33349.html" title="律吕调阳"><img src="/images/blogs/%E5%8F%A4%E5%85%B8%E6%99%BA%E6%85%A7/%E5%BE%8B%E5%90%95%E8%B0%83%E9%98%B3/%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA%E5%A4%B4%E5%9B%BE.png" onerror="this.onerror=null;this.src='/images/ui/404.jpg'" alt="律吕调阳"/></a><div class="content"><a class="title" href="/post/33349.html" title="律吕调阳">律吕调阳</a><time datetime="2025-11-27T14:00:00.000Z" title="发表于 2025-11-27 22:00:00">2025-11-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2026 By Scharfsinnig</div><div class="footer_custom_text">嗨~不肉，welcome to my blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><!-- load lazyload before main.js so LazyLoad class exists when main initializes--><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0.27/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@9.1.2/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadGiscus () {
  let nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'light'

  const config = Object.assign({
    src: 'https://giscus.app/client.js',
    'data-repo': 'Scharfsinnig/scharfsinnig.github.io',
    'data-repo-id': 'R_kgDOHp7n3A',
    'data-category-id': 'DIC_kwDOHp7n3M4Cvyzj',
    'data-mapping': 'pathname',
    'data-theme': nowTheme,
    'data-reactions-enabled': '1',
    crossorigin: 'anonymous',
    async: true
  },{"data-lang":"zh-CN","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-category":"Announcements"})

  let ele = document.createElement('script')
  for (let key in config) {
    ele.setAttribute(key, config[key])
  }
  document.getElementById('giscus-wrap').insertAdjacentElement('afterbegin',ele)
}

function changeGiscusTheme () {
  const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'light'

  function sendMessage(message) {
    const iframe = document.querySelector('iframe.giscus-frame');
    if (!iframe) return;
    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
  }

  sendMessage({
    setConfig: {
      theme: theme
    }
  });
}

if ('Giscus' === 'Giscus' || !false) {
  if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
  else loadGiscus()
} else {
  function loadOtherComment () {
    loadGiscus()
  }
}</script></div><div id="player-flyout" aria-hidden="true"><div id="player-mount"></div></div><script src="/js/custom/player-flyout.js"></script><script src="/js/custom/now-playing.js"></script><script src="/js/custom/flink-enhance.js"></script><script src="/js/custom/mermaid-modal-simple.js"></script><script src="/js/custom/hero-avatar-scroll.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="102,126,234" opacity="0.3" zIndex="-1" count="66" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="💖,✨,🌟,💫,🎉,🚀" data-fontsize="18px" data-random="true" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["link[rel=\"canonical\"]","meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script src="/js/tech-effects.js"></script><script>(function(){
  function swapLazyImages(){
    try{
      var imgs = document.querySelectorAll('img[data-lazy-src]');
      imgs.forEach(function(img){
        var real = img.getAttribute('data-lazy-src');
        if (real && img.src !== real) {
          img.src = real;
          img.removeAttribute('data-lazy-src');
        }
      });
    } catch(e){}
  }
  function schedule(){
    swapLazyImages();
    var tries = 0;
    var timer = setInterval(function(){
      tries++;
      swapLazyImages();
      if (tries >= 10 || !document.querySelector('img[data-lazy-src]')) {
        clearInterval(timer);
      }
    }, 500);
  }
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', schedule);
  } else {
    schedule();
  }
  window.addEventListener('load', swapLazyImages);
})();</script></div></body></html>